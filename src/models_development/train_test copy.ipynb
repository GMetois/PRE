{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "# A module to print a model summary (outputs shape, number of parameters, ...)\n",
    "import torchsummary\n",
    "\n",
    "# TensorBoard for visualization\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data to be used\n",
    "DATASET = \"../../datasets/dataset_mask/\"\n",
    "\n",
    "\n",
    "class TraversabilityDataset(Dataset):\n",
    "    \"\"\"Custom Dataset class to represent our dataset\n",
    "    It includes data and information about the data\n",
    "\n",
    "    Args:\n",
    "        Dataset (class): Abstract class which represents a dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, traversal_costs_file, images_directory,\n",
    "                 transform=None):\n",
    "        \"\"\"Constructor of the class\n",
    "\n",
    "        Args:\n",
    "            traversal_costs_file (string): Path to the csv file which contains\n",
    "            images index and their associated traversal cost\n",
    "            images_directory (string): Directory with all the images\n",
    "            transform (callable, optional): Transforms to be applied on a\n",
    "            sample. Defaults to None.\n",
    "        \"\"\"\n",
    "        # Read the csv file\n",
    "        self.traversal_costs_frame = pd.read_csv(traversal_costs_file)\n",
    "        \n",
    "        # Initialize the name of the images directory\n",
    "        self.images_directory = images_directory\n",
    "        \n",
    "        # Initialize the transforms\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the dataset\n",
    "\n",
    "        Returns:\n",
    "            int: Number of samples\n",
    "        \"\"\"\n",
    "        return len(self.traversal_costs_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Allow to access a sample by its index\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of a sample\n",
    "\n",
    "        Returns:\n",
    "            list: Sample at index idx\n",
    "            ([image, traversal_cost])\n",
    "        \"\"\"\n",
    "        # Get the image name at index idx\n",
    "        image_name = os.path.join(self.images_directory,\n",
    "                                  self.traversal_costs_frame.iloc[idx, 0])\n",
    "        \n",
    "        # Read the image\n",
    "        image = Image.open(image_name)\n",
    "        \n",
    "        # Get the corresponding traversal cost\n",
    "        traversal_cost = self.traversal_costs_frame.iloc[idx, 1:]\n",
    "        traversal_cost = np.array(traversal_cost)\n",
    "        traversal_cost = np.float32(traversal_cost)\n",
    "\n",
    "        # Create the sample\n",
    "        sample = [image, traversal_cost]\n",
    "\n",
    "        # Eventually apply transforms to the image\n",
    "        if self.transform:\n",
    "            sample[0] = self.transform(sample[0])\n",
    "\n",
    "        return sample\n",
    " \n",
    "\n",
    "# Compose several transforms together to be applied to training data\n",
    "# (Note that transforms are not applied yet)\n",
    "train_transform = transforms.Compose([\n",
    "    # Reduce the size of the images\n",
    "    # (if size is an int, the smaller edge of the\n",
    "    # image will be matched to this number and the ration is kept)\n",
    "    # transforms.Resize(100),\n",
    "    transforms.Resize((70, 210)),\n",
    "    \n",
    "    # Crop the image at the center\n",
    "    # (if size is an int, a square crop is made)\n",
    "    # transforms.CenterCrop(100),\n",
    "    \n",
    "    # Crop a random square in the image\n",
    "    # transforms.RandomCrop(100),\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    # transforms.Grayscale(num_output_channels=1),\n",
    "    \n",
    "    # Perform horizontal flip of the image with a probability of 0.5\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    \n",
    "    # Convert a PIL Image or numpy.ndarray to tensor\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Normalize a tensor image with pre-computed mean and standard deviation\n",
    "    # (based on the data used to train the model(s))\n",
    "    # (be careful, it only works on torch.*Tensor)\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Define a different set of transforms testing\n",
    "# (for instance we do not need to flip the image)\n",
    "test_transform = transforms.Compose([\n",
    "    # transforms.Resize(100),\n",
    "    transforms.Resize((70, 210)),\n",
    "    # transforms.Grayscale(),\n",
    "    # transforms.CenterCrop(100),\n",
    "    # transforms.RandomCrop(100),\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Mean and standard deviation were pre-computed on the training data\n",
    "    # (on the ImageNet dataset)\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "\n",
    "# Create a Dataset instance for our training data\n",
    "data = TraversabilityDataset(\n",
    "    traversal_costs_file=DATASET+\"traversal_costs.csv\",\n",
    "    images_directory=DATASET+\"zed_node_rgb_image_rect_color\",\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "# Split our training dataset into a training dataset and a validation dataset\n",
    "train_set, val_set, test_set = random_split(data, [0.8, 0.1, 0.1])\n",
    "\n",
    "\n",
    "# Combine a dataset and a sampler, and provide an iterable over the dataset\n",
    "# (setting shuffle argument to true calls a RandomSampler, and avoids to\n",
    "# have to create a Sampler object)\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_size=16,\n",
    "    shuffle=False,  # SequentialSampler\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO: why is not the GPU available?\n",
    "# Use a GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model design and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1030, -0.0698,  0.0523,  0.0767, -0.0279,  0.1014, -0.0228, -0.0600,\n",
       "         -0.0196,  0.0485,  0.1054, -0.0579, -0.0161,  0.1065,  0.0377,  0.0629,\n",
       "          0.0477, -0.0449, -0.0515, -0.0854, -0.0138,  0.0551, -0.0288, -0.0713,\n",
       "         -0.0659,  0.0506,  0.1028,  0.1011,  0.0099,  0.0969, -0.0553, -0.0211,\n",
       "         -0.0551, -0.0347, -0.0473, -0.0615, -0.0174,  0.0409,  0.0456,  0.0025,\n",
       "         -0.0667, -0.0927, -0.0307,  0.0074, -0.0174,  0.0939, -0.0649,  0.0942,\n",
       "         -0.0326, -0.0166, -0.0791,  0.0787,  0.0470, -0.0592, -0.0632, -0.1071,\n",
       "         -0.1031,  0.0180,  0.0033,  0.1026, -0.0963,  0.0357,  0.0129,  0.0663,\n",
       "          0.0430, -0.0344, -0.0620, -0.0146,  0.0578,  0.0382,  0.0234, -0.0792,\n",
       "          0.0756,  0.0915, -0.0151,  0.0074,  0.0734, -0.0689,  0.0805, -0.0206,\n",
       "          0.0576,  0.0067, -0.0513,  0.0570, -0.0772,  0.1032,  0.0173, -0.0908,\n",
       "         -0.0857,  0.0985,  0.0058, -0.0025, -0.0704, -0.0663, -0.0121, -0.0454,\n",
       "         -0.0245,  0.0179,  0.0385, -0.1023,  0.0621,  0.0816,  0.0815,  0.0396,\n",
       "         -0.0659, -0.0438, -0.0144,  0.0722,  0.0780,  0.0154, -0.0451, -0.0841,\n",
       "         -0.0606, -0.0583, -0.0886, -0.0470, -0.0642, -0.0981,  0.0137, -0.0747,\n",
       "          0.0462, -0.0358, -0.0206, -0.0837,  0.0403,  0.0630,  0.0937,  0.0662,\n",
       "          0.0966,  0.0274,  0.0882,  0.0675,  0.0244, -0.0693, -0.0565, -0.0153,\n",
       "          0.0054, -0.0233,  0.0167, -0.0991, -0.0384,  0.0942, -0.0748,  0.0222,\n",
       "         -0.0421, -0.0830, -0.0068, -0.0904, -0.0590,  0.0866, -0.0689,  0.0790,\n",
       "         -0.0761,  0.0848,  0.0228,  0.0780,  0.0084,  0.0650,  0.0605, -0.0813,\n",
       "          0.0537, -0.0945,  0.1039,  0.1010, -0.0652, -0.0778, -0.0021, -0.0881,\n",
       "          0.0564, -0.0977,  0.0010,  0.0802, -0.0473, -0.0541,  0.0788,  0.0761,\n",
       "         -0.0713,  0.1027,  0.0780, -0.0820, -0.1045, -0.0957, -0.0820,  0.0683,\n",
       "          0.0123,  0.1038,  0.0788,  0.0296, -0.0886,  0.0661,  0.1035,  0.0881,\n",
       "         -0.0815, -0.0178,  0.0863, -0.0751, -0.0712, -0.0219, -0.0366, -0.0593,\n",
       "         -0.0372, -0.0040,  0.0048, -0.0801,  0.0921, -0.0536, -0.0189, -0.0959,\n",
       "         -0.0229, -0.0962,  0.0795, -0.0552,  0.0294, -0.0905, -0.0311, -0.0556,\n",
       "          0.0332,  0.0232,  0.0114,  0.0057,  0.0141, -0.0552, -0.0594, -0.0877,\n",
       "          0.0054,  0.0824,  0.0468, -0.0938,  0.0049, -0.0421, -0.0001,  0.0569,\n",
       "          0.0366, -0.0847, -0.0082,  0.0104,  0.0330, -0.0418,  0.0310, -0.0559,\n",
       "          0.0325, -0.1022,  0.0651,  0.0350,  0.0383, -0.0284,  0.0968, -0.0715,\n",
       "         -0.0183, -0.0421, -0.1078, -0.0864, -0.0500, -0.0778, -0.0099,  0.0767,\n",
       "         -0.0132, -0.0128, -0.0200,  0.0776,  0.0344,  0.0427, -0.0468, -0.0613,\n",
       "          0.0113, -0.0355, -0.1021,  0.1073, -0.0829, -0.0462, -0.0811, -0.0601,\n",
       "         -0.0834, -0.0518,  0.0736, -0.0035, -0.0709,  0.0583, -0.0116, -0.0425,\n",
       "          0.0014, -0.0361,  0.0465,  0.0274,  0.0491, -0.1009,  0.0834, -0.0070,\n",
       "          0.0555, -0.0379, -0.0338, -0.0270,  0.0550,  0.0807, -0.0951,  0.0442,\n",
       "          0.0120,  0.0114, -0.0988,  0.0990, -0.0163,  0.0431,  0.0519, -0.0750,\n",
       "          0.0378,  0.0996,  0.0408, -0.0301, -0.0657,  0.0211,  0.0033, -0.0563,\n",
       "          0.0179, -0.0997, -0.0453, -0.0288, -0.0057,  0.0323, -0.0556,  0.0760,\n",
       "          0.0329, -0.0862,  0.0546, -0.0284,  0.1014, -0.0123,  0.1061,  0.0697,\n",
       "         -0.0496, -0.0833,  0.0677,  0.0990, -0.0749, -0.0825,  0.0478,  0.0352,\n",
       "         -0.0440,  0.0717,  0.0741,  0.0573, -0.0083,  0.0623, -0.0019,  0.0982,\n",
       "          0.1017, -0.0672,  0.0460, -0.0249,  0.0163, -0.0779, -0.0335,  0.0826,\n",
       "         -0.0818, -0.0648,  0.0330, -0.0745, -0.0857, -0.0639, -0.0726, -0.0896,\n",
       "         -0.0560,  0.0976, -0.0402,  0.0753, -0.0111,  0.0108,  0.0083, -0.0791,\n",
       "         -0.0736,  0.0213, -0.0460, -0.0080, -0.0813, -0.0875, -0.0329, -0.0097,\n",
       "          0.0367, -0.0933,  0.0040,  0.0183,  0.0621,  0.0063,  0.0244, -0.0954,\n",
       "          0.1036, -0.0087, -0.0826,  0.0372,  0.1076, -0.0839, -0.0611,  0.0182,\n",
       "          0.0518,  0.0552, -0.0464,  0.0685,  0.0333,  0.1031, -0.0355,  0.0720,\n",
       "          0.0672,  0.0697,  0.0944, -0.0130, -0.0710, -0.0690,  0.0340, -0.0805,\n",
       "          0.0913,  0.0706, -0.0361, -0.0239, -0.0636,  0.0059,  0.0789, -0.0413,\n",
       "         -0.0548, -0.0374, -0.0622, -0.0640,  0.0890,  0.0206, -0.0529, -0.0986,\n",
       "          0.0564, -0.0537,  0.0450, -0.1050, -0.0528,  0.0824, -0.0282, -0.0491,\n",
       "          0.0551, -0.0380,  0.0219,  0.0029, -0.0814, -0.0329, -0.0680,  0.0369,\n",
       "         -0.0786, -0.0297,  0.0321, -0.0706,  0.0458, -0.0929, -0.0773, -0.0399,\n",
       "         -0.0937,  0.0070,  0.0443,  0.1044, -0.0104, -0.0145, -0.0845, -0.0968,\n",
       "         -0.0847, -0.0684, -0.0851,  0.0007,  0.0824, -0.0878, -0.0342,  0.0972,\n",
       "          0.0432,  0.0043, -0.1012, -0.0564,  0.0995,  0.0233, -0.0223,  0.0638,\n",
       "         -0.0120, -0.0272,  0.0864, -0.0294, -0.0968,  0.0482,  0.0408, -0.0016,\n",
       "         -0.0220, -0.0296,  0.0387, -0.0907,  0.0255, -0.0701, -0.0251, -0.1017,\n",
       "         -0.0637, -0.0812, -0.0666, -0.0327,  0.0478,  0.0832, -0.0507,  0.1046,\n",
       "         -0.1020, -0.0325, -0.0904,  0.1017,  0.0434, -0.1053,  0.0092,  0.0204,\n",
       "          0.0083,  0.0186,  0.0296, -0.0603, -0.0549,  0.0843,  0.0863, -0.0299]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open TensorBoard\n",
    "tensorboard = SummaryWriter()\n",
    "\n",
    "# Load the pre-trained AlexNet model\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT).to(device=device)\n",
    "\n",
    "# Replace the last layer by a fully-connected one with 1 output\n",
    "model.fc = nn.Linear(model.fc.in_features, 1, device=device)\n",
    "\n",
    "# print(next(model.fc.parameters()).device)\n",
    "# print(next(model.parameters()).device)\n",
    "\n",
    "# Display the architecture in TensorBoard\n",
    "images, traversal_scores = next(iter(train_loader))\n",
    "images = images.to(device)\n",
    "# print(images.device)\n",
    "tensorboard.add_graph(model, images)\n",
    "\n",
    "# print(model)\n",
    "# print(torchsummary.summary(model, (3, 100, 100)))\n",
    "\n",
    "# Initialize the last layer using Xavier initialization\n",
    "nn.init.xavier_uniform_(model.fc.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conv1.weight', torch.Size([64, 3, 7, 7])),\n",
       " ('bn1.weight', torch.Size([64])),\n",
       " ('bn1.bias', torch.Size([64])),\n",
       " ('layer1.0.conv1.weight', torch.Size([64, 64, 3, 3])),\n",
       " ('layer1.0.bn1.weight', torch.Size([64])),\n",
       " ('layer1.0.bn1.bias', torch.Size([64])),\n",
       " ('layer1.0.conv2.weight', torch.Size([64, 64, 3, 3])),\n",
       " ('layer1.0.bn2.weight', torch.Size([64])),\n",
       " ('layer1.0.bn2.bias', torch.Size([64])),\n",
       " ('layer1.1.conv1.weight', torch.Size([64, 64, 3, 3])),\n",
       " ('layer1.1.bn1.weight', torch.Size([64])),\n",
       " ('layer1.1.bn1.bias', torch.Size([64])),\n",
       " ('layer1.1.conv2.weight', torch.Size([64, 64, 3, 3])),\n",
       " ('layer1.1.bn2.weight', torch.Size([64])),\n",
       " ('layer1.1.bn2.bias', torch.Size([64])),\n",
       " ('layer2.0.conv1.weight', torch.Size([128, 64, 3, 3])),\n",
       " ('layer2.0.bn1.weight', torch.Size([128])),\n",
       " ('layer2.0.bn1.bias', torch.Size([128])),\n",
       " ('layer2.0.conv2.weight', torch.Size([128, 128, 3, 3])),\n",
       " ('layer2.0.bn2.weight', torch.Size([128])),\n",
       " ('layer2.0.bn2.bias', torch.Size([128])),\n",
       " ('layer2.0.downsample.0.weight', torch.Size([128, 64, 1, 1])),\n",
       " ('layer2.0.downsample.1.weight', torch.Size([128])),\n",
       " ('layer2.0.downsample.1.bias', torch.Size([128])),\n",
       " ('layer2.1.conv1.weight', torch.Size([128, 128, 3, 3])),\n",
       " ('layer2.1.bn1.weight', torch.Size([128])),\n",
       " ('layer2.1.bn1.bias', torch.Size([128])),\n",
       " ('layer2.1.conv2.weight', torch.Size([128, 128, 3, 3])),\n",
       " ('layer2.1.bn2.weight', torch.Size([128])),\n",
       " ('layer2.1.bn2.bias', torch.Size([128])),\n",
       " ('layer3.0.conv1.weight', torch.Size([256, 128, 3, 3])),\n",
       " ('layer3.0.bn1.weight', torch.Size([256])),\n",
       " ('layer3.0.bn1.bias', torch.Size([256])),\n",
       " ('layer3.0.conv2.weight', torch.Size([256, 256, 3, 3])),\n",
       " ('layer3.0.bn2.weight', torch.Size([256])),\n",
       " ('layer3.0.bn2.bias', torch.Size([256])),\n",
       " ('layer3.0.downsample.0.weight', torch.Size([256, 128, 1, 1])),\n",
       " ('layer3.0.downsample.1.weight', torch.Size([256])),\n",
       " ('layer3.0.downsample.1.bias', torch.Size([256])),\n",
       " ('layer3.1.conv1.weight', torch.Size([256, 256, 3, 3])),\n",
       " ('layer3.1.bn1.weight', torch.Size([256])),\n",
       " ('layer3.1.bn1.bias', torch.Size([256])),\n",
       " ('layer3.1.conv2.weight', torch.Size([256, 256, 3, 3])),\n",
       " ('layer3.1.bn2.weight', torch.Size([256])),\n",
       " ('layer3.1.bn2.bias', torch.Size([256])),\n",
       " ('layer4.0.conv1.weight', torch.Size([512, 256, 3, 3])),\n",
       " ('layer4.0.bn1.weight', torch.Size([512])),\n",
       " ('layer4.0.bn1.bias', torch.Size([512])),\n",
       " ('layer4.0.conv2.weight', torch.Size([512, 512, 3, 3])),\n",
       " ('layer4.0.bn2.weight', torch.Size([512])),\n",
       " ('layer4.0.bn2.bias', torch.Size([512])),\n",
       " ('layer4.0.downsample.0.weight', torch.Size([512, 256, 1, 1])),\n",
       " ('layer4.0.downsample.1.weight', torch.Size([512])),\n",
       " ('layer4.0.downsample.1.bias', torch.Size([512])),\n",
       " ('layer4.1.conv1.weight', torch.Size([512, 512, 3, 3])),\n",
       " ('layer4.1.bn1.weight', torch.Size([512])),\n",
       " ('layer4.1.bn1.bias', torch.Size([512])),\n",
       " ('layer4.1.conv2.weight', torch.Size([512, 512, 3, 3])),\n",
       " ('layer4.1.bn2.weight', torch.Size([512])),\n",
       " ('layer4.1.bn2.bias', torch.Size([512])),\n",
       " ('fc.weight', torch.Size([1, 512])),\n",
       " ('fc.bias', torch.Size([1]))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(name, param.shape) for name, param in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.1030, -0.0698,  0.0523,  0.0767, -0.0279,  0.1014, -0.0228, -0.0600,\n",
      "         -0.0196,  0.0485,  0.1054, -0.0579, -0.0161,  0.1065,  0.0377,  0.0629,\n",
      "          0.0477, -0.0449, -0.0515, -0.0854, -0.0138,  0.0551, -0.0288, -0.0713,\n",
      "         -0.0659,  0.0506,  0.1028,  0.1011,  0.0099,  0.0969, -0.0553, -0.0211,\n",
      "         -0.0551, -0.0347, -0.0473, -0.0615, -0.0174,  0.0409,  0.0456,  0.0025,\n",
      "         -0.0667, -0.0927, -0.0307,  0.0074, -0.0174,  0.0939, -0.0649,  0.0942,\n",
      "         -0.0326, -0.0166, -0.0791,  0.0787,  0.0470, -0.0592, -0.0632, -0.1071,\n",
      "         -0.1031,  0.0180,  0.0033,  0.1026, -0.0963,  0.0357,  0.0129,  0.0663,\n",
      "          0.0430, -0.0344, -0.0620, -0.0146,  0.0578,  0.0382,  0.0234, -0.0792,\n",
      "          0.0756,  0.0915, -0.0151,  0.0074,  0.0734, -0.0689,  0.0805, -0.0206,\n",
      "          0.0576,  0.0067, -0.0513,  0.0570, -0.0772,  0.1032,  0.0173, -0.0908,\n",
      "         -0.0857,  0.0985,  0.0058, -0.0025, -0.0704, -0.0663, -0.0121, -0.0454,\n",
      "         -0.0245,  0.0179,  0.0385, -0.1023,  0.0621,  0.0816,  0.0815,  0.0396,\n",
      "         -0.0659, -0.0438, -0.0144,  0.0722,  0.0780,  0.0154, -0.0451, -0.0841,\n",
      "         -0.0606, -0.0583, -0.0886, -0.0470, -0.0642, -0.0981,  0.0137, -0.0747,\n",
      "          0.0462, -0.0358, -0.0206, -0.0837,  0.0403,  0.0630,  0.0937,  0.0662,\n",
      "          0.0966,  0.0274,  0.0882,  0.0675,  0.0244, -0.0693, -0.0565, -0.0153,\n",
      "          0.0054, -0.0233,  0.0167, -0.0991, -0.0384,  0.0942, -0.0748,  0.0222,\n",
      "         -0.0421, -0.0830, -0.0068, -0.0904, -0.0590,  0.0866, -0.0689,  0.0790,\n",
      "         -0.0761,  0.0848,  0.0228,  0.0780,  0.0084,  0.0650,  0.0605, -0.0813,\n",
      "          0.0537, -0.0945,  0.1039,  0.1010, -0.0652, -0.0778, -0.0021, -0.0881,\n",
      "          0.0564, -0.0977,  0.0010,  0.0802, -0.0473, -0.0541,  0.0788,  0.0761,\n",
      "         -0.0713,  0.1027,  0.0780, -0.0820, -0.1045, -0.0957, -0.0820,  0.0683,\n",
      "          0.0123,  0.1038,  0.0788,  0.0296, -0.0886,  0.0661,  0.1035,  0.0881,\n",
      "         -0.0815, -0.0178,  0.0863, -0.0751, -0.0712, -0.0219, -0.0366, -0.0593,\n",
      "         -0.0372, -0.0040,  0.0048, -0.0801,  0.0921, -0.0536, -0.0189, -0.0959,\n",
      "         -0.0229, -0.0962,  0.0795, -0.0552,  0.0294, -0.0905, -0.0311, -0.0556,\n",
      "          0.0332,  0.0232,  0.0114,  0.0057,  0.0141, -0.0552, -0.0594, -0.0877,\n",
      "          0.0054,  0.0824,  0.0468, -0.0938,  0.0049, -0.0421, -0.0001,  0.0569,\n",
      "          0.0366, -0.0847, -0.0082,  0.0104,  0.0330, -0.0418,  0.0310, -0.0559,\n",
      "          0.0325, -0.1022,  0.0651,  0.0350,  0.0383, -0.0284,  0.0968, -0.0715,\n",
      "         -0.0183, -0.0421, -0.1078, -0.0864, -0.0500, -0.0778, -0.0099,  0.0767,\n",
      "         -0.0132, -0.0128, -0.0200,  0.0776,  0.0344,  0.0427, -0.0468, -0.0613,\n",
      "          0.0113, -0.0355, -0.1021,  0.1073, -0.0829, -0.0462, -0.0811, -0.0601,\n",
      "         -0.0834, -0.0518,  0.0736, -0.0035, -0.0709,  0.0583, -0.0116, -0.0425,\n",
      "          0.0014, -0.0361,  0.0465,  0.0274,  0.0491, -0.1009,  0.0834, -0.0070,\n",
      "          0.0555, -0.0379, -0.0338, -0.0270,  0.0550,  0.0807, -0.0951,  0.0442,\n",
      "          0.0120,  0.0114, -0.0988,  0.0990, -0.0163,  0.0431,  0.0519, -0.0750,\n",
      "          0.0378,  0.0996,  0.0408, -0.0301, -0.0657,  0.0211,  0.0033, -0.0563,\n",
      "          0.0179, -0.0997, -0.0453, -0.0288, -0.0057,  0.0323, -0.0556,  0.0760,\n",
      "          0.0329, -0.0862,  0.0546, -0.0284,  0.1014, -0.0123,  0.1061,  0.0697,\n",
      "         -0.0496, -0.0833,  0.0677,  0.0990, -0.0749, -0.0825,  0.0478,  0.0352,\n",
      "         -0.0440,  0.0717,  0.0741,  0.0573, -0.0083,  0.0623, -0.0019,  0.0982,\n",
      "          0.1017, -0.0672,  0.0460, -0.0249,  0.0163, -0.0779, -0.0335,  0.0826,\n",
      "         -0.0818, -0.0648,  0.0330, -0.0745, -0.0857, -0.0639, -0.0726, -0.0896,\n",
      "         -0.0560,  0.0976, -0.0402,  0.0753, -0.0111,  0.0108,  0.0083, -0.0791,\n",
      "         -0.0736,  0.0213, -0.0460, -0.0080, -0.0813, -0.0875, -0.0329, -0.0097,\n",
      "          0.0367, -0.0933,  0.0040,  0.0183,  0.0621,  0.0063,  0.0244, -0.0954,\n",
      "          0.1036, -0.0087, -0.0826,  0.0372,  0.1076, -0.0839, -0.0611,  0.0182,\n",
      "          0.0518,  0.0552, -0.0464,  0.0685,  0.0333,  0.1031, -0.0355,  0.0720,\n",
      "          0.0672,  0.0697,  0.0944, -0.0130, -0.0710, -0.0690,  0.0340, -0.0805,\n",
      "          0.0913,  0.0706, -0.0361, -0.0239, -0.0636,  0.0059,  0.0789, -0.0413,\n",
      "         -0.0548, -0.0374, -0.0622, -0.0640,  0.0890,  0.0206, -0.0529, -0.0986,\n",
      "          0.0564, -0.0537,  0.0450, -0.1050, -0.0528,  0.0824, -0.0282, -0.0491,\n",
      "          0.0551, -0.0380,  0.0219,  0.0029, -0.0814, -0.0329, -0.0680,  0.0369,\n",
      "         -0.0786, -0.0297,  0.0321, -0.0706,  0.0458, -0.0929, -0.0773, -0.0399,\n",
      "         -0.0937,  0.0070,  0.0443,  0.1044, -0.0104, -0.0145, -0.0845, -0.0968,\n",
      "         -0.0847, -0.0684, -0.0851,  0.0007,  0.0824, -0.0878, -0.0342,  0.0972,\n",
      "          0.0432,  0.0043, -0.1012, -0.0564,  0.0995,  0.0233, -0.0223,  0.0638,\n",
      "         -0.0120, -0.0272,  0.0864, -0.0294, -0.0968,  0.0482,  0.0408, -0.0016,\n",
      "         -0.0220, -0.0296,  0.0387, -0.0907,  0.0255, -0.0701, -0.0251, -0.1017,\n",
      "         -0.0637, -0.0812, -0.0666, -0.0327,  0.0478,  0.0832, -0.0507,  0.1046,\n",
      "         -0.1020, -0.0325, -0.0904,  0.1017,  0.0434, -0.1053,  0.0092,  0.0204,\n",
      "          0.0083,  0.0186,  0.0296, -0.0603, -0.0549,  0.0843,  0.0863, -0.0299]],\n",
      "       device='cuda:0')), ('bias', tensor([0.0048], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "print(model.fc.state_dict())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 [train]: 100%|██████████| 26/26 [00:02<00:00, 10.69batch/s, batch_loss=1.21e+3]\n",
      "Epoch 0 [val]: 100%|██████████| 4/4 [00:00<00:00, 20.63batch/s, batch_loss=1.01e+10]\n",
      "Epoch 1 [train]: 100%|██████████| 26/26 [00:02<00:00, 12.35batch/s, batch_loss=2.88e+3]\n",
      "Epoch 1 [val]: 100%|██████████| 4/4 [00:00<00:00, 20.89batch/s, batch_loss=1.98e+4]\n",
      "Epoch 2 [train]: 100%|██████████| 26/26 [00:02<00:00, 12.11batch/s, batch_loss=168]    \n",
      "Epoch 2 [val]: 100%|██████████| 4/4 [00:00<00:00, 23.03batch/s, batch_loss=120]\n",
      "Epoch 3 [train]: 100%|██████████| 26/26 [00:02<00:00, 12.73batch/s, batch_loss=294]    \n",
      "Epoch 3 [val]: 100%|██████████| 4/4 [00:00<00:00, 23.68batch/s, batch_loss=70.5]\n",
      "Epoch 4 [train]: 100%|██████████| 26/26 [00:02<00:00, 12.50batch/s, batch_loss=3.22e+3]\n",
      "Epoch 4 [val]: 100%|██████████| 4/4 [00:00<00:00, 20.86batch/s, batch_loss=54.7]\n",
      "Epoch 5 [train]: 100%|██████████| 26/26 [00:02<00:00, 12.82batch/s, batch_loss=216]    \n",
      "Epoch 5 [val]: 100%|██████████| 4/4 [00:00<00:00, 21.89batch/s, batch_loss=155]\n",
      "Epoch 6 [train]: 100%|██████████| 26/26 [00:01<00:00, 13.83batch/s, batch_loss=748]    \n",
      "Epoch 6 [val]: 100%|██████████| 4/4 [00:00<00:00, 22.57batch/s, batch_loss=437]\n",
      "Epoch 7 [train]: 100%|██████████| 26/26 [00:02<00:00, 12.05batch/s, batch_loss=485]    \n",
      "Epoch 7 [val]: 100%|██████████| 4/4 [00:00<00:00, 21.36batch/s, batch_loss=60.5]\n",
      "Epoch 8 [train]: 100%|██████████| 26/26 [00:01<00:00, 13.29batch/s, batch_loss=1.19e+3]\n",
      "Epoch 8 [val]: 100%|██████████| 4/4 [00:00<00:00, 19.17batch/s, batch_loss=1.09e+3]\n",
      "Epoch 9 [train]: 100%|██████████| 26/26 [00:02<00:00, 12.55batch/s, batch_loss=71.6]   \n",
      "Epoch 9 [val]: 100%|██████████| 4/4 [00:00<00:00, 21.62batch/s, batch_loss=141]\n",
      "Epoch 10 [train]: 100%|██████████| 26/26 [00:02<00:00, 12.58batch/s, batch_loss=277]    \n",
      "Epoch 10 [val]: 100%|██████████| 4/4 [00:00<00:00, 22.34batch/s, batch_loss=53] \n",
      "Epoch 11 [train]: 100%|██████████| 26/26 [00:01<00:00, 13.34batch/s, batch_loss=110]    \n",
      "Epoch 11 [val]: 100%|██████████| 4/4 [00:00<00:00, 21.86batch/s, batch_loss=45.7]\n",
      "Epoch 12 [train]: 100%|██████████| 26/26 [00:01<00:00, 13.01batch/s, batch_loss=141]    \n",
      "Epoch 12 [val]: 100%|██████████| 4/4 [00:00<00:00, 16.71batch/s, batch_loss=691]\n",
      "Epoch 13 [train]: 100%|██████████| 26/26 [00:02<00:00, 11.49batch/s, batch_loss=4.95e+3]\n",
      "Epoch 13 [val]: 100%|██████████| 4/4 [00:00<00:00, 22.52batch/s, batch_loss=587]    \n",
      "Epoch 14 [train]: 100%|██████████| 26/26 [00:02<00:00, 12.15batch/s, batch_loss=117]    \n",
      "Epoch 14 [val]: 100%|██████████| 4/4 [00:00<00:00, 22.19batch/s, batch_loss=22.2]   \n",
      "Epoch 15 [train]: 100%|██████████| 26/26 [00:02<00:00, 12.34batch/s, batch_loss=25.1]   \n",
      "Epoch 15 [val]: 100%|██████████| 4/4 [00:00<00:00, 20.55batch/s, batch_loss=877]\n",
      "Epoch 16 [train]: 100%|██████████| 26/26 [00:01<00:00, 13.48batch/s, batch_loss=2.07e+3]\n",
      "Epoch 16 [val]: 100%|██████████| 4/4 [00:00<00:00, 22.91batch/s, batch_loss=170]\n",
      "Epoch 17 [train]: 100%|██████████| 26/26 [00:01<00:00, 13.17batch/s, batch_loss=565]    \n",
      "Epoch 17 [val]: 100%|██████████| 4/4 [00:00<00:00, 19.71batch/s, batch_loss=45.4]\n",
      "Epoch 18 [train]: 100%|██████████| 26/26 [00:01<00:00, 13.06batch/s, batch_loss=2.32e+3]\n",
      "Epoch 18 [val]: 100%|██████████| 4/4 [00:00<00:00, 22.62batch/s, batch_loss=3.3]    \n",
      "Epoch 19 [train]: 100%|██████████| 26/26 [00:02<00:00, 12.88batch/s, batch_loss=4.6e+3] \n",
      "Epoch 19 [val]: 100%|██████████| 4/4 [00:00<00:00, 22.29batch/s, batch_loss=516]\n",
      "Epoch 20 [train]: 100%|██████████| 26/26 [00:02<00:00, 12.89batch/s, batch_loss=836]    \n",
      "Epoch 20 [val]: 100%|██████████| 4/4 [00:00<00:00, 22.16batch/s, batch_loss=31.9]\n",
      "Epoch 21 [train]: 100%|██████████| 26/26 [00:02<00:00, 12.15batch/s, batch_loss=590]    \n",
      "Epoch 21 [val]: 100%|██████████| 4/4 [00:00<00:00, 22.32batch/s, batch_loss=209]\n",
      "Epoch 22 [train]: 100%|██████████| 26/26 [00:02<00:00, 12.56batch/s, batch_loss=286]    \n",
      "Epoch 22 [val]: 100%|██████████| 4/4 [00:00<00:00, 23.09batch/s, batch_loss=36] \n",
      "Epoch 23 [train]: 100%|██████████| 26/26 [00:01<00:00, 13.15batch/s, batch_loss=1.07e+3]\n",
      "Epoch 23 [val]: 100%|██████████| 4/4 [00:00<00:00, 19.44batch/s, batch_loss=214] \n",
      "Epoch 24 [train]: 100%|██████████| 26/26 [00:01<00:00, 13.81batch/s, batch_loss=302]    \n",
      "Epoch 24 [val]: 100%|██████████| 4/4 [00:00<00:00, 21.29batch/s, batch_loss=118]   \n",
      "Epoch 25 [train]: 100%|██████████| 26/26 [00:02<00:00, 12.70batch/s, batch_loss=151]    \n",
      "Epoch 25 [val]: 100%|██████████| 4/4 [00:00<00:00, 21.74batch/s, batch_loss=33.8]\n",
      "Epoch 26 [train]: 100%|██████████| 26/26 [00:01<00:00, 13.58batch/s, batch_loss=338]    \n",
      "Epoch 26 [val]: 100%|██████████| 4/4 [00:00<00:00, 28.63batch/s, batch_loss=870]    \n",
      "Epoch 27 [train]: 100%|██████████| 26/26 [00:02<00:00, 12.26batch/s, batch_loss=963]    \n",
      "Epoch 27 [val]: 100%|██████████| 4/4 [00:00<00:00, 21.51batch/s, batch_loss=13.5]\n",
      "Epoch 28 [train]: 100%|██████████| 26/26 [00:01<00:00, 14.34batch/s, batch_loss=133]    \n",
      "Epoch 28 [val]: 100%|██████████| 4/4 [00:00<00:00, 19.58batch/s, batch_loss=13.9]\n",
      "Epoch 29 [train]: 100%|██████████| 26/26 [00:01<00:00, 13.10batch/s, batch_loss=142]    \n",
      "Epoch 29 [val]: 100%|██████████| 4/4 [00:00<00:00, 22.74batch/s, batch_loss=186]\n",
      "Epoch 30 [train]: 100%|██████████| 26/26 [00:01<00:00, 13.01batch/s, batch_loss=1.4e+3] \n",
      "Epoch 30 [val]: 100%|██████████| 4/4 [00:00<00:00, 23.31batch/s, batch_loss=1.34e+3]\n",
      "Epoch 31 [train]: 100%|██████████| 26/26 [00:01<00:00, 14.01batch/s, batch_loss=133]   \n",
      "Epoch 31 [val]: 100%|██████████| 4/4 [00:00<00:00, 20.64batch/s, batch_loss=21.9]\n",
      "Epoch 32 [train]: 100%|██████████| 26/26 [00:02<00:00, 12.63batch/s, batch_loss=91.5]   \n",
      "Epoch 32 [val]: 100%|██████████| 4/4 [00:00<00:00, 20.81batch/s, batch_loss=84.3]   \n",
      "Epoch 33 [train]: 100%|██████████| 26/26 [00:02<00:00, 12.55batch/s, batch_loss=2.22e+3]\n",
      "Epoch 33 [val]: 100%|██████████| 4/4 [00:00<00:00, 19.21batch/s, batch_loss=242]    \n",
      "Epoch 34 [train]: 100%|██████████| 26/26 [00:02<00:00, 12.12batch/s, batch_loss=368]    \n",
      "Epoch 34 [val]: 100%|██████████| 4/4 [00:00<00:00, 20.28batch/s, batch_loss=60.3]\n",
      "Epoch 35 [train]: 100%|██████████| 26/26 [00:02<00:00, 12.84batch/s, batch_loss=140]    \n",
      "Epoch 35 [val]: 100%|██████████| 4/4 [00:00<00:00, 18.26batch/s, batch_loss=118] \n",
      "Epoch 36 [train]: 100%|██████████| 26/26 [00:01<00:00, 13.03batch/s, batch_loss=329]    \n",
      "Epoch 36 [val]: 100%|██████████| 4/4 [00:00<00:00, 22.41batch/s, batch_loss=286]\n",
      "Epoch 37 [train]: 100%|██████████| 26/26 [00:01<00:00, 13.05batch/s, batch_loss=139]    \n",
      "Epoch 37 [val]: 100%|██████████| 4/4 [00:00<00:00, 20.44batch/s, batch_loss=1e+3]\n",
      "Epoch 38 [train]: 100%|██████████| 26/26 [00:02<00:00, 12.77batch/s, batch_loss=4.61e+3]\n",
      "Epoch 38 [val]: 100%|██████████| 4/4 [00:00<00:00, 23.41batch/s, batch_loss=246]\n",
      "Epoch 39 [train]: 100%|██████████| 26/26 [00:01<00:00, 13.39batch/s, batch_loss=88.7]   \n",
      "Epoch 39 [val]: 100%|██████████| 4/4 [00:00<00:00, 24.76batch/s, batch_loss=32.6]\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "base_params = [param for name, param in model.named_parameters()\n",
    "        if name not in [\"fc.weight\", \"fc.bias\"]]\n",
    "\n",
    "optimizer = optim.SGD([\n",
    "    {\"params\": base_params},\n",
    "    {\"params\": model.fc.parameters(), \"lr\": 1e-3},\n",
    "],\n",
    "    lr=1e-4, momentum=0.9, weight_decay=0.001)\n",
    "\n",
    "# An epoch is one complete pass of the training dataset through the network\n",
    "NB_EPOCHS = 40\n",
    "\n",
    "# Loop over the epochs\n",
    "for epoch in range(NB_EPOCHS):\n",
    "    \n",
    "    # Training\n",
    "    train_loss = 0.\n",
    "    \n",
    "    # Configure the model for training\n",
    "    # (good practice, only necessary if the model operates differently for\n",
    "    # training and validation)\n",
    "    model.train()\n",
    "    \n",
    "    # Add a progress bar\n",
    "    train_loader_pbar = tqdm(train_loader, unit=\"batch\")\n",
    "    \n",
    "    # Loop over the training batches\n",
    "    for images, traversal_scores in train_loader_pbar:\n",
    "        \n",
    "        # Print the epoch and training mode\n",
    "        train_loader_pbar.set_description(f\"Epoch {epoch} [train]\")\n",
    "        \n",
    "        # Move images and traversal scores to GPU (if available)\n",
    "        images = images.to(device)\n",
    "        traversal_scores = traversal_scores.to(device)\n",
    "        \n",
    "        # Zero out gradients before each backpropagation pass, to avoid that\n",
    "        # they accumulate\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform forward pass\n",
    "        predicted_traversal_scores = model(images)\n",
    "        \n",
    "        # Compute loss \n",
    "        loss = criterion(predicted_traversal_scores, traversal_scores)\n",
    "        \n",
    "        # Print the batch loss next to the progress bar\n",
    "        train_loader_pbar.set_postfix(batch_loss=loss.item())\n",
    "        \n",
    "        # Perform backpropagation (compute gradients)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Adjust parameters based on gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate batch loss to average over the epoch\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    # Validation\n",
    "    val_loss = 0.\n",
    "    \n",
    "    # Configure the model for testing\n",
    "    model.eval()\n",
    "    \n",
    "    # Add a progress bar\n",
    "    val_loader_pbar = tqdm(val_loader, unit=\"batch\")\n",
    "    \n",
    "    # Loop over the validation batches\n",
    "    for images, traversal_scores in val_loader_pbar:\n",
    "        \n",
    "        # Print the epoch and validation mode\n",
    "        val_loader_pbar.set_description(f\"Epoch {epoch} [val]\")\n",
    "        \n",
    "        # Move images and traversal scores to GPU (if available)\n",
    "        images = images.to(device)\n",
    "        traversal_scores = traversal_scores.to(device)\n",
    "        \n",
    "        # Perform forward pass (only, no backpropagation)\n",
    "        predicted_traversal_scores = model(images)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(predicted_traversal_scores, traversal_scores)\n",
    "        \n",
    "        # Print the batch loss next to the progress bar\n",
    "        val_loader_pbar.set_postfix(batch_loss=loss.item())\n",
    "        \n",
    "        # Accumulate batch loss to average over the epoch\n",
    "        val_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    # # Display the computed losses\n",
    "    # print(f\"Epoch {epoch}: Train Loss: {train_loss/len(train_loader)}\\\n",
    "    #       Validation Loss: {val_loss/len(val_loader)}\")\n",
    "    # loss_values[0, epoch] = train_loss/len(train_loader)\n",
    "    # loss_values[1, epoch] = val_loss/len(val_loader)\n",
    "    \n",
    "    # Add the losses to TensorBoard\n",
    "    tensorboard.add_scalar(\"train_loss\", train_loss/len(train_loader), epoch)\n",
    "    tensorboard.add_scalar(\"val_loss\", val_loss/len(val_loader), epoch)\n",
    "\n",
    "# Close TensorBoard\n",
    "tensorboard.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 279.5303897857666\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_loss = 0.\n",
    "\n",
    "denormalize = transforms.Compose([\n",
    "    transforms.Normalize(\n",
    "        mean=[0., 0., 0.],\n",
    "        std=[1/0.229, 1/0.224, 1/0.225]\n",
    "    ),\n",
    "    transforms.Normalize(\n",
    "        mean=[-0.485, -0.456, -0.406],\n",
    "        std=[1., 1., 1.]\n",
    "    ),\n",
    "    transforms.ToPILImage(),\n",
    "])\n",
    "\n",
    "# Loop over the testing batches\n",
    "for images, traversal_scores in test_loader:\n",
    "    \n",
    "    images = images.to(device)\n",
    "    traversal_scores = traversal_scores.to(device)\n",
    "    \n",
    "    # Perform forward pass\n",
    "    predicted_traversal_scores = model(images)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(predicted_traversal_scores, traversal_scores)\n",
    "    \n",
    "    # Accumulate batch loss to average of the entire testing set\n",
    "    test_loss += loss.item()\n",
    "    # print(loss.item())\n",
    "    \n",
    "    # for i in range(images.shape[0]):\n",
    "    #     plt.imshow(denormalize(images[i]))\n",
    "\n",
    "print(f\"Test loss: {test_loss/len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      " tensor([[ 4.1697],\n",
      "        [-3.5956],\n",
      "        [28.1200],\n",
      "        [-1.6885],\n",
      "        [ 0.6041],\n",
      "        [18.9100],\n",
      "        [19.9780],\n",
      "        [ 6.9668],\n",
      "        [21.8091],\n",
      "        [22.6206],\n",
      "        [-3.5112],\n",
      "        [49.5338],\n",
      "        [ 0.2201],\n",
      "        [-0.4538],\n",
      "        [-1.5574],\n",
      "        [19.6026]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Ground truth:\n",
      " tensor([[ 5.5662],\n",
      "        [ 2.8388],\n",
      "        [58.9338],\n",
      "        [ 4.6169],\n",
      "        [ 5.3382],\n",
      "        [21.8468],\n",
      "        [16.7859],\n",
      "        [ 9.9833],\n",
      "        [13.8517],\n",
      "        [ 8.2817],\n",
      "        [ 3.8169],\n",
      "        [22.9494],\n",
      "        [ 7.3710],\n",
      "        [ 4.3341],\n",
      "        [ 3.9473],\n",
      "        [23.7088]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "images, traversal_scores = next(iter(test_loader))\n",
    "\n",
    "images = images.to(device)\n",
    "traversal_scores = traversal_scores.to(device)\n",
    "\n",
    "predicted_traversal_scores = model(images)\n",
    "\n",
    "print(\"Prediction:\\n\", predicted_traversal_scores)\n",
    "print(\"Ground truth:\\n\", traversal_scores)\n",
    "\n",
    "# print(predicted_traversal_scores-traversal_scores)\n",
    "\n",
    "# predicted_traversal_scores = predicted_traversal_scores.to(\"cpu\").detach().numpy()\n",
    "# plt.hist(predicted_traversal_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([45.,  4.,  3.,  0.,  0.,  1.,  0.,  0.,  0.,  1.]),\n",
       " array([  1.8573602,  29.70956  ,  57.56176  ,  85.41396  , 113.26616  ,\n",
       "        141.11836  , 168.97057  , 196.82277  , 224.67496  , 252.52716  ,\n",
       "        280.37936  ], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traversal_scores_train = []\n",
    "\n",
    "for _, score in val_set:\n",
    "    traversal_scores_train.append(score[0])\n",
    "    \n",
    "# print(traversal_scores_train)\n",
    "plt.hist(traversal_scores_train, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(44*16-697)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model parameters\n",
    "torch.save(model.state_dict(), \"resnet18_fine_tuned_small_bag2.params\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
