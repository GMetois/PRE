{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "# A module to print a model summary (outputs shape, number of parameters, ...)\n",
    "import torchsummary\n",
    "\n",
    "# TensorBoard for visualization\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data to be used\n",
    "DATASET = \"../../datasets/dataset_path_grass_robust/\"\n",
    "\n",
    "\n",
    "class TraversabilityDataset(Dataset):\n",
    "    \"\"\"Custom Dataset class to represent our dataset\n",
    "    It includes data and information about the data\n",
    "\n",
    "    Args:\n",
    "        Dataset (class): Abstract class which represents a dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, traversal_costs_file, images_directory,\n",
    "                 transform=None):\n",
    "        \"\"\"Constructor of the class\n",
    "\n",
    "        Args:\n",
    "            traversal_costs_file (string): Path to the csv file which contains\n",
    "            images index and their associated traversal cost\n",
    "            images_directory (string): Directory with all the images\n",
    "            transform (callable, optional): Transforms to be applied on a\n",
    "            sample. Defaults to None.\n",
    "        \"\"\"\n",
    "        # Read the csv file\n",
    "        self.traversal_costs_frame = pd.read_csv(traversal_costs_file)\n",
    "        \n",
    "        # Initialize the name of the images directory\n",
    "        self.images_directory = images_directory\n",
    "        \n",
    "        # Initialize the transforms\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the dataset\n",
    "\n",
    "        Returns:\n",
    "            int: Number of samples\n",
    "        \"\"\"\n",
    "        return len(self.traversal_costs_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Allow to access a sample by its index\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of a sample\n",
    "\n",
    "        Returns:\n",
    "            list: Sample at index idx\n",
    "            ([image, traversal_cost])\n",
    "        \"\"\"\n",
    "        # Get the image name at index idx\n",
    "        image_name = os.path.join(self.images_directory,\n",
    "                                  self.traversal_costs_frame.iloc[idx, 0])\n",
    "        \n",
    "        # Read the image\n",
    "        image = Image.open(image_name)\n",
    "        \n",
    "        # Get the corresponding traversal cost\n",
    "        traversal_cost = self.traversal_costs_frame.iloc[idx, 1:]\n",
    "        traversal_cost = np.array(traversal_cost)\n",
    "        traversal_cost = np.float32(traversal_cost)\n",
    "\n",
    "        # Create the sample\n",
    "        sample = [image, traversal_cost]\n",
    "\n",
    "        # Eventually apply transforms to the image\n",
    "        if self.transform:\n",
    "            sample[0] = self.transform(sample[0])\n",
    "\n",
    "        return sample\n",
    " \n",
    "\n",
    "# Compose several transforms together to be applied to training data\n",
    "# (Note that transforms are not applied yet)\n",
    "train_transform = transforms.Compose([\n",
    "    # Reduce the size of the images\n",
    "    # (if size is an int, the smaller edge of the\n",
    "    # image will be matched to this number and the ration is kept)\n",
    "    # transforms.Resize(100),\n",
    "    transforms.Resize((70, 210)),\n",
    "    \n",
    "    # Crop the image at the center\n",
    "    # (if size is an int, a square crop is made)\n",
    "    # transforms.CenterCrop(100),\n",
    "    \n",
    "    # Crop a random square in the image\n",
    "    # transforms.RandomCrop(100),\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    # transforms.Grayscale(num_output_channels=1),\n",
    "    \n",
    "    # Perform horizontal flip of the image with a probability of 0.5\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    \n",
    "    # Convert a PIL Image or numpy.ndarray to tensor\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Normalize a tensor image with pre-computed mean and standard deviation\n",
    "    # (based on the data used to train the model(s))\n",
    "    # (be careful, it only works on torch.*Tensor)\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Define a different set of transforms testing\n",
    "# (for instance we do not need to flip the image)\n",
    "test_transform = transforms.Compose([\n",
    "    # transforms.Resize(100),\n",
    "    transforms.Resize((70, 210)),\n",
    "    # transforms.Grayscale(),\n",
    "    # transforms.CenterCrop(100),\n",
    "    # transforms.RandomCrop(100),\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Mean and standard deviation were pre-computed on the training data\n",
    "    # (on the ImageNet dataset)\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "\n",
    "# Create a Dataset instance for our training data\n",
    "data = TraversabilityDataset(\n",
    "    traversal_costs_file=DATASET+\"traversal_costs.csv\",\n",
    "    images_directory=DATASET+\"images\",\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "# Split our training dataset into a training dataset and a validation dataset\n",
    "train_set, val_set, test_set = random_split(data, [0.8, 0.1, 0.1])\n",
    "\n",
    "\n",
    "# Combine a dataset and a sampler, and provide an iterable over the dataset\n",
    "# (setting shuffle argument to true calls a RandomSampler, and avoids to\n",
    "# have to create a Sampler object)\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_size=16,\n",
    "    shuffle=False,  # SequentialSampler\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO: why is not the GPU available?\n",
    "# Use a GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model design and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-8.7366e-02, -8.7466e-02, -9.1127e-02, -2.7945e-02, -2.4400e-02,\n",
       "          5.5187e-02, -3.9936e-02, -3.9100e-02, -3.9264e-02,  4.7483e-02,\n",
       "         -2.8778e-02, -2.6954e-02, -2.6294e-02,  5.8885e-02,  8.8925e-02,\n",
       "          9.5054e-02,  7.1811e-02,  7.0792e-02,  3.2471e-02,  2.8301e-02,\n",
       "         -1.3810e-02, -2.3731e-02, -7.6416e-02, -1.3214e-02, -7.1473e-02,\n",
       "          6.7732e-02,  9.8464e-02,  7.5054e-02, -1.0597e-01, -1.0000e-02,\n",
       "         -2.7847e-02, -2.0060e-02,  8.2244e-02,  1.6317e-02,  6.8060e-02,\n",
       "         -3.3019e-02, -8.1388e-02,  1.0433e-01, -2.3224e-02,  5.2776e-02,\n",
       "         -2.5341e-03, -6.0108e-02,  7.1667e-02,  3.8420e-02,  2.9831e-02,\n",
       "          9.7914e-03,  8.9555e-02, -2.3987e-02,  1.6149e-02,  3.4221e-02,\n",
       "          6.7928e-02, -4.1634e-02,  4.7592e-03, -3.3577e-02, -5.4274e-02,\n",
       "          9.5666e-02, -2.3600e-02,  3.0338e-02, -1.0328e-01,  4.9331e-02,\n",
       "          2.3290e-02, -4.4305e-02, -8.4479e-02,  8.2275e-02, -1.0227e-01,\n",
       "         -1.1420e-02, -9.2629e-02,  5.5133e-02, -1.2883e-02,  1.0278e-01,\n",
       "         -2.6048e-02, -8.5092e-02, -4.9426e-02, -4.6192e-02, -5.8321e-02,\n",
       "          6.5159e-03,  1.0170e-01, -6.2281e-02, -6.0418e-02,  9.7103e-02,\n",
       "         -2.0664e-04, -2.2632e-02, -1.6164e-02, -3.6959e-02,  1.0157e-02,\n",
       "          1.0070e-01, -8.3636e-03, -2.6032e-02,  1.0197e-01,  9.0952e-02,\n",
       "         -8.0762e-02, -6.1347e-02,  4.6722e-02, -1.0963e-02, -1.0312e-01,\n",
       "          5.9080e-02, -8.7374e-02, -5.7627e-02,  2.4585e-02,  7.4560e-02,\n",
       "         -2.9123e-02,  5.2826e-02, -9.1422e-02, -1.0766e-01,  2.8936e-02,\n",
       "          7.9839e-02, -9.0473e-02, -1.0226e-02,  5.2438e-02,  8.4624e-05,\n",
       "          1.4802e-02,  5.2590e-02, -8.7895e-02,  7.0058e-02,  5.6588e-02,\n",
       "          2.7092e-02, -2.5904e-02, -6.6496e-02, -5.6837e-02, -5.7743e-02,\n",
       "         -6.2217e-02,  9.6441e-02,  2.0940e-02, -2.7623e-02,  5.3022e-03,\n",
       "         -5.7776e-02,  9.1597e-02,  6.1957e-02, -1.7875e-02, -7.2035e-03,\n",
       "          2.7300e-02,  1.0782e-01, -5.6885e-02, -8.4113e-02,  1.3194e-02,\n",
       "          1.1733e-02, -3.2214e-02, -2.9551e-02,  8.5983e-02, -1.0479e-01,\n",
       "         -8.1495e-04,  6.9641e-02, -5.6751e-03, -9.8735e-05,  1.0351e-01,\n",
       "         -7.4008e-02,  8.3842e-02,  1.0527e-02,  9.9392e-02, -1.0082e-01,\n",
       "         -8.2008e-02, -6.1946e-02,  3.0914e-02, -2.8005e-02, -5.3420e-02,\n",
       "          7.6256e-02,  5.8483e-02, -6.4429e-02, -2.0197e-02, -7.3034e-06,\n",
       "          8.2034e-02,  9.8812e-02, -8.4194e-02,  6.1188e-03,  8.5306e-02,\n",
       "         -8.1506e-02,  7.4349e-02, -7.9953e-02, -4.9291e-02,  4.3610e-02,\n",
       "         -4.6239e-03, -1.2085e-02,  7.6960e-03,  8.1395e-03, -2.3484e-02,\n",
       "         -7.5496e-03, -3.5518e-02,  1.7546e-02, -6.4203e-02,  5.8932e-02,\n",
       "         -9.5076e-02,  5.4733e-03,  9.0435e-02, -1.9603e-02,  1.0338e-01,\n",
       "          7.3145e-02, -1.0651e-01, -2.7752e-02,  6.4103e-02, -1.0325e-01,\n",
       "         -4.1314e-02, -7.7824e-02,  7.2676e-02, -9.5903e-02, -6.7833e-02,\n",
       "         -1.0288e-01, -2.6295e-02, -3.9772e-02,  8.8161e-02,  5.4824e-02,\n",
       "          8.4600e-02, -1.0095e-01,  9.3616e-02, -1.8155e-02, -9.7206e-02,\n",
       "         -3.9374e-02,  6.4871e-02, -6.0310e-02, -6.3972e-02,  7.4069e-02,\n",
       "          8.3655e-02,  2.2854e-02,  1.0758e-01, -4.8318e-02,  3.4849e-02,\n",
       "          7.2305e-02, -8.9048e-02,  2.2675e-02, -8.6345e-02,  2.6884e-02,\n",
       "          1.0345e-01, -9.1055e-02, -6.5053e-02,  9.2483e-02, -3.3157e-02,\n",
       "          4.6243e-02,  3.2320e-02, -1.9098e-02, -6.2904e-02, -1.9613e-03,\n",
       "          5.9851e-02, -3.5468e-02, -1.0000e-01,  8.3961e-02,  1.7412e-02,\n",
       "         -3.3955e-02, -4.2393e-03,  9.6287e-02,  9.6422e-04,  1.0128e-01,\n",
       "          9.8256e-02, -8.5918e-02, -1.1838e-02, -5.0469e-02,  8.0893e-03,\n",
       "          4.8704e-03,  1.7973e-02,  1.0108e-01, -4.5268e-02, -3.8983e-02,\n",
       "         -6.2415e-02,  1.0759e-01, -3.2837e-02, -6.8561e-02, -9.7366e-02,\n",
       "          1.2368e-02, -8.9979e-02,  9.1153e-02,  2.4158e-02,  8.6663e-02,\n",
       "          9.0132e-02,  9.3734e-02, -7.4002e-02, -9.0223e-02,  7.6774e-02,\n",
       "          6.3596e-02,  4.6887e-02, -8.3879e-02, -2.0575e-02, -5.2755e-03,\n",
       "         -4.0727e-02, -8.2409e-02,  9.0230e-02, -9.5811e-02,  3.4441e-03,\n",
       "         -4.0208e-02,  6.1549e-02,  3.4055e-02,  9.0756e-02,  4.4617e-02,\n",
       "         -7.9246e-02,  8.0965e-02,  2.7419e-02,  8.6576e-02, -7.9812e-02,\n",
       "          5.9625e-02,  3.3152e-02, -9.2108e-02, -5.5823e-02, -9.6413e-02,\n",
       "         -9.1236e-02,  3.2771e-02, -3.8434e-02,  4.6908e-02,  6.9696e-02,\n",
       "          2.7985e-03,  7.7836e-03,  8.8102e-02, -8.8428e-02, -1.0716e-01,\n",
       "          3.6613e-02, -3.6071e-02, -7.0305e-02, -6.1536e-02,  6.3312e-02,\n",
       "          1.3959e-02, -8.0568e-02,  6.5229e-02,  1.1368e-02,  3.1178e-02,\n",
       "          4.1034e-02,  2.5743e-02,  4.9328e-02,  3.1372e-02,  1.9083e-02,\n",
       "         -7.4029e-03,  4.0904e-02,  2.6813e-02,  9.4049e-02, -1.0621e-01,\n",
       "          1.9187e-02,  9.2334e-02, -2.2998e-02,  4.1980e-03,  3.1522e-02,\n",
       "          1.0209e-01, -1.0262e-01, -2.3082e-02,  3.9045e-02,  1.0135e-01,\n",
       "         -5.9961e-02,  4.3928e-02,  1.8729e-02,  9.2089e-02,  7.3255e-02,\n",
       "         -5.7475e-02, -1.0406e-01, -3.1206e-02,  9.5330e-02,  1.0643e-01,\n",
       "         -7.5634e-02, -5.9979e-02,  3.7854e-02, -2.4688e-02, -9.5038e-02,\n",
       "         -8.8308e-02,  6.0602e-02, -6.0301e-02,  4.6142e-02,  4.1799e-02,\n",
       "          4.7902e-02, -6.4161e-02,  8.8896e-03, -1.0484e-01,  3.5341e-02,\n",
       "         -1.0701e-01, -3.7717e-02,  2.5533e-02,  7.2821e-02,  2.2764e-02,\n",
       "          4.0299e-02, -5.0536e-02, -6.7735e-02, -4.5872e-02, -5.3376e-02,\n",
       "         -6.7982e-02,  1.0909e-02, -6.9482e-02,  7.8946e-02, -6.2348e-02,\n",
       "         -7.7052e-02,  4.2645e-02,  8.4781e-02, -2.6800e-02, -8.7252e-02,\n",
       "          5.5613e-03,  1.7751e-02,  1.0456e-01, -6.0884e-02, -7.9088e-02,\n",
       "          1.0396e-01, -4.2767e-02,  9.9602e-02, -1.8826e-02, -8.3841e-02,\n",
       "         -1.4578e-02, -2.3003e-02, -2.4414e-02, -6.3080e-02, -3.7980e-02,\n",
       "         -8.4114e-02, -2.6510e-02,  1.0714e-01,  9.2956e-02, -7.5382e-02,\n",
       "         -7.1955e-02, -9.9416e-02,  1.0059e-01, -6.6939e-02, -1.3343e-02,\n",
       "         -9.2446e-02, -7.3426e-02,  3.4292e-02, -6.1926e-03, -1.0221e-01,\n",
       "         -7.2000e-02,  9.1751e-02, -2.0468e-02,  1.1471e-02,  9.1863e-02,\n",
       "          9.0514e-02, -1.0317e-02,  3.5814e-02, -1.0019e-01,  1.4552e-02,\n",
       "          3.8370e-02,  4.7075e-02,  3.1395e-02, -5.8571e-02, -1.0666e-01,\n",
       "          3.3000e-02,  1.7342e-02, -7.3838e-02, -1.0322e-01, -5.3685e-03,\n",
       "         -4.3290e-02, -1.0767e-01, -9.3536e-03, -5.0622e-02,  1.0555e-01,\n",
       "         -9.5153e-02, -6.9477e-02, -1.2355e-02, -7.0557e-02, -1.2388e-02,\n",
       "         -7.5594e-02, -6.4847e-02, -8.7803e-02, -7.6449e-02,  3.0314e-02,\n",
       "         -3.1117e-03,  4.6227e-02, -3.9543e-02, -3.2457e-02, -8.4600e-02,\n",
       "         -8.8802e-03,  4.0098e-02,  4.3195e-02,  9.3620e-02,  9.3825e-02,\n",
       "          8.3744e-03, -5.5640e-02, -9.8072e-02,  5.0510e-02,  1.5377e-02,\n",
       "          5.1516e-02, -3.2162e-02, -4.1641e-02, -9.6876e-02,  2.3241e-02,\n",
       "         -3.6276e-03, -6.8338e-02, -5.5738e-02, -7.2887e-02,  5.5277e-02,\n",
       "          1.3294e-02,  5.0750e-02, -3.7551e-02, -8.8432e-02,  1.0331e-01,\n",
       "          2.8633e-02,  2.6973e-02,  9.7633e-03,  8.9798e-02,  6.6499e-02,\n",
       "          6.1044e-02,  2.6019e-02,  7.7472e-02, -3.8097e-02,  8.7602e-03,\n",
       "          6.8182e-02,  5.7112e-02,  5.1098e-02,  8.6484e-02,  7.7295e-02,\n",
       "          4.3590e-02,  1.0787e-01, -1.0619e-01, -6.0892e-02, -1.5593e-02,\n",
       "         -1.0218e-01, -6.9343e-02,  7.9880e-03, -2.9443e-02,  1.8945e-02,\n",
       "         -8.8997e-02, -3.8422e-02, -8.5770e-02,  1.0300e-01,  5.6192e-02,\n",
       "         -4.6663e-02,  5.1501e-02, -2.5226e-03,  6.5033e-02,  6.7038e-02,\n",
       "         -4.1119e-02,  1.0192e-01,  4.6991e-04,  1.0546e-01,  1.9462e-02,\n",
       "          9.2060e-02,  5.4239e-02]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open TensorBoard\n",
    "tensorboard = SummaryWriter()\n",
    "\n",
    "# Load the pre-trained AlexNet model\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT).to(device=device)\n",
    "\n",
    "# Replace the last layer by a fully-connected one with 1 output\n",
    "model.fc = nn.Linear(model.fc.in_features, 1, device=device)\n",
    "\n",
    "# print(next(model.fc.parameters()).device)\n",
    "# print(next(model.parameters()).device)\n",
    "\n",
    "# Display the architecture in TensorBoard\n",
    "images, traversal_scores = next(iter(train_loader))\n",
    "images = images.to(device)\n",
    "# print(images.device)\n",
    "tensorboard.add_graph(model, images)\n",
    "\n",
    "# print(model)\n",
    "# print(torchsummary.summary(model, (3, 100, 100)))\n",
    "\n",
    "# Initialize the last layer using Xavier initialization\n",
    "nn.init.xavier_uniform_(model.fc.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conv1.weight', torch.Size([64, 3, 7, 7])),\n",
       " ('bn1.weight', torch.Size([64])),\n",
       " ('bn1.bias', torch.Size([64])),\n",
       " ('layer1.0.conv1.weight', torch.Size([64, 64, 3, 3])),\n",
       " ('layer1.0.bn1.weight', torch.Size([64])),\n",
       " ('layer1.0.bn1.bias', torch.Size([64])),\n",
       " ('layer1.0.conv2.weight', torch.Size([64, 64, 3, 3])),\n",
       " ('layer1.0.bn2.weight', torch.Size([64])),\n",
       " ('layer1.0.bn2.bias', torch.Size([64])),\n",
       " ('layer1.1.conv1.weight', torch.Size([64, 64, 3, 3])),\n",
       " ('layer1.1.bn1.weight', torch.Size([64])),\n",
       " ('layer1.1.bn1.bias', torch.Size([64])),\n",
       " ('layer1.1.conv2.weight', torch.Size([64, 64, 3, 3])),\n",
       " ('layer1.1.bn2.weight', torch.Size([64])),\n",
       " ('layer1.1.bn2.bias', torch.Size([64])),\n",
       " ('layer2.0.conv1.weight', torch.Size([128, 64, 3, 3])),\n",
       " ('layer2.0.bn1.weight', torch.Size([128])),\n",
       " ('layer2.0.bn1.bias', torch.Size([128])),\n",
       " ('layer2.0.conv2.weight', torch.Size([128, 128, 3, 3])),\n",
       " ('layer2.0.bn2.weight', torch.Size([128])),\n",
       " ('layer2.0.bn2.bias', torch.Size([128])),\n",
       " ('layer2.0.downsample.0.weight', torch.Size([128, 64, 1, 1])),\n",
       " ('layer2.0.downsample.1.weight', torch.Size([128])),\n",
       " ('layer2.0.downsample.1.bias', torch.Size([128])),\n",
       " ('layer2.1.conv1.weight', torch.Size([128, 128, 3, 3])),\n",
       " ('layer2.1.bn1.weight', torch.Size([128])),\n",
       " ('layer2.1.bn1.bias', torch.Size([128])),\n",
       " ('layer2.1.conv2.weight', torch.Size([128, 128, 3, 3])),\n",
       " ('layer2.1.bn2.weight', torch.Size([128])),\n",
       " ('layer2.1.bn2.bias', torch.Size([128])),\n",
       " ('layer3.0.conv1.weight', torch.Size([256, 128, 3, 3])),\n",
       " ('layer3.0.bn1.weight', torch.Size([256])),\n",
       " ('layer3.0.bn1.bias', torch.Size([256])),\n",
       " ('layer3.0.conv2.weight', torch.Size([256, 256, 3, 3])),\n",
       " ('layer3.0.bn2.weight', torch.Size([256])),\n",
       " ('layer3.0.bn2.bias', torch.Size([256])),\n",
       " ('layer3.0.downsample.0.weight', torch.Size([256, 128, 1, 1])),\n",
       " ('layer3.0.downsample.1.weight', torch.Size([256])),\n",
       " ('layer3.0.downsample.1.bias', torch.Size([256])),\n",
       " ('layer3.1.conv1.weight', torch.Size([256, 256, 3, 3])),\n",
       " ('layer3.1.bn1.weight', torch.Size([256])),\n",
       " ('layer3.1.bn1.bias', torch.Size([256])),\n",
       " ('layer3.1.conv2.weight', torch.Size([256, 256, 3, 3])),\n",
       " ('layer3.1.bn2.weight', torch.Size([256])),\n",
       " ('layer3.1.bn2.bias', torch.Size([256])),\n",
       " ('layer4.0.conv1.weight', torch.Size([512, 256, 3, 3])),\n",
       " ('layer4.0.bn1.weight', torch.Size([512])),\n",
       " ('layer4.0.bn1.bias', torch.Size([512])),\n",
       " ('layer4.0.conv2.weight', torch.Size([512, 512, 3, 3])),\n",
       " ('layer4.0.bn2.weight', torch.Size([512])),\n",
       " ('layer4.0.bn2.bias', torch.Size([512])),\n",
       " ('layer4.0.downsample.0.weight', torch.Size([512, 256, 1, 1])),\n",
       " ('layer4.0.downsample.1.weight', torch.Size([512])),\n",
       " ('layer4.0.downsample.1.bias', torch.Size([512])),\n",
       " ('layer4.1.conv1.weight', torch.Size([512, 512, 3, 3])),\n",
       " ('layer4.1.bn1.weight', torch.Size([512])),\n",
       " ('layer4.1.bn1.bias', torch.Size([512])),\n",
       " ('layer4.1.conv2.weight', torch.Size([512, 512, 3, 3])),\n",
       " ('layer4.1.bn2.weight', torch.Size([512])),\n",
       " ('layer4.1.bn2.bias', torch.Size([512])),\n",
       " ('fc.weight', torch.Size([1, 512])),\n",
       " ('fc.bias', torch.Size([1]))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(name, param.shape) for name, param in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-8.7366e-02, -8.7466e-02, -9.1127e-02, -2.7945e-02, -2.4400e-02,\n",
      "          5.5187e-02, -3.9936e-02, -3.9100e-02, -3.9264e-02,  4.7483e-02,\n",
      "         -2.8778e-02, -2.6954e-02, -2.6294e-02,  5.8885e-02,  8.8925e-02,\n",
      "          9.5054e-02,  7.1811e-02,  7.0792e-02,  3.2471e-02,  2.8301e-02,\n",
      "         -1.3810e-02, -2.3731e-02, -7.6416e-02, -1.3214e-02, -7.1473e-02,\n",
      "          6.7732e-02,  9.8464e-02,  7.5054e-02, -1.0597e-01, -1.0000e-02,\n",
      "         -2.7847e-02, -2.0060e-02,  8.2244e-02,  1.6317e-02,  6.8060e-02,\n",
      "         -3.3019e-02, -8.1388e-02,  1.0433e-01, -2.3224e-02,  5.2776e-02,\n",
      "         -2.5341e-03, -6.0108e-02,  7.1667e-02,  3.8420e-02,  2.9831e-02,\n",
      "          9.7914e-03,  8.9555e-02, -2.3987e-02,  1.6149e-02,  3.4221e-02,\n",
      "          6.7928e-02, -4.1634e-02,  4.7592e-03, -3.3577e-02, -5.4274e-02,\n",
      "          9.5666e-02, -2.3600e-02,  3.0338e-02, -1.0328e-01,  4.9331e-02,\n",
      "          2.3290e-02, -4.4305e-02, -8.4479e-02,  8.2275e-02, -1.0227e-01,\n",
      "         -1.1420e-02, -9.2629e-02,  5.5133e-02, -1.2883e-02,  1.0278e-01,\n",
      "         -2.6048e-02, -8.5092e-02, -4.9426e-02, -4.6192e-02, -5.8321e-02,\n",
      "          6.5159e-03,  1.0170e-01, -6.2281e-02, -6.0418e-02,  9.7103e-02,\n",
      "         -2.0664e-04, -2.2632e-02, -1.6164e-02, -3.6959e-02,  1.0157e-02,\n",
      "          1.0070e-01, -8.3636e-03, -2.6032e-02,  1.0197e-01,  9.0952e-02,\n",
      "         -8.0762e-02, -6.1347e-02,  4.6722e-02, -1.0963e-02, -1.0312e-01,\n",
      "          5.9080e-02, -8.7374e-02, -5.7627e-02,  2.4585e-02,  7.4560e-02,\n",
      "         -2.9123e-02,  5.2826e-02, -9.1422e-02, -1.0766e-01,  2.8936e-02,\n",
      "          7.9839e-02, -9.0473e-02, -1.0226e-02,  5.2438e-02,  8.4624e-05,\n",
      "          1.4802e-02,  5.2590e-02, -8.7895e-02,  7.0058e-02,  5.6588e-02,\n",
      "          2.7092e-02, -2.5904e-02, -6.6496e-02, -5.6837e-02, -5.7743e-02,\n",
      "         -6.2217e-02,  9.6441e-02,  2.0940e-02, -2.7623e-02,  5.3022e-03,\n",
      "         -5.7776e-02,  9.1597e-02,  6.1957e-02, -1.7875e-02, -7.2035e-03,\n",
      "          2.7300e-02,  1.0782e-01, -5.6885e-02, -8.4113e-02,  1.3194e-02,\n",
      "          1.1733e-02, -3.2214e-02, -2.9551e-02,  8.5983e-02, -1.0479e-01,\n",
      "         -8.1495e-04,  6.9641e-02, -5.6751e-03, -9.8735e-05,  1.0351e-01,\n",
      "         -7.4008e-02,  8.3842e-02,  1.0527e-02,  9.9392e-02, -1.0082e-01,\n",
      "         -8.2008e-02, -6.1946e-02,  3.0914e-02, -2.8005e-02, -5.3420e-02,\n",
      "          7.6256e-02,  5.8483e-02, -6.4429e-02, -2.0197e-02, -7.3034e-06,\n",
      "          8.2034e-02,  9.8812e-02, -8.4194e-02,  6.1188e-03,  8.5306e-02,\n",
      "         -8.1506e-02,  7.4349e-02, -7.9953e-02, -4.9291e-02,  4.3610e-02,\n",
      "         -4.6239e-03, -1.2085e-02,  7.6960e-03,  8.1395e-03, -2.3484e-02,\n",
      "         -7.5496e-03, -3.5518e-02,  1.7546e-02, -6.4203e-02,  5.8932e-02,\n",
      "         -9.5076e-02,  5.4733e-03,  9.0435e-02, -1.9603e-02,  1.0338e-01,\n",
      "          7.3145e-02, -1.0651e-01, -2.7752e-02,  6.4103e-02, -1.0325e-01,\n",
      "         -4.1314e-02, -7.7824e-02,  7.2676e-02, -9.5903e-02, -6.7833e-02,\n",
      "         -1.0288e-01, -2.6295e-02, -3.9772e-02,  8.8161e-02,  5.4824e-02,\n",
      "          8.4600e-02, -1.0095e-01,  9.3616e-02, -1.8155e-02, -9.7206e-02,\n",
      "         -3.9374e-02,  6.4871e-02, -6.0310e-02, -6.3972e-02,  7.4069e-02,\n",
      "          8.3655e-02,  2.2854e-02,  1.0758e-01, -4.8318e-02,  3.4849e-02,\n",
      "          7.2305e-02, -8.9048e-02,  2.2675e-02, -8.6345e-02,  2.6884e-02,\n",
      "          1.0345e-01, -9.1055e-02, -6.5053e-02,  9.2483e-02, -3.3157e-02,\n",
      "          4.6243e-02,  3.2320e-02, -1.9098e-02, -6.2904e-02, -1.9613e-03,\n",
      "          5.9851e-02, -3.5468e-02, -1.0000e-01,  8.3961e-02,  1.7412e-02,\n",
      "         -3.3955e-02, -4.2393e-03,  9.6287e-02,  9.6422e-04,  1.0128e-01,\n",
      "          9.8256e-02, -8.5918e-02, -1.1838e-02, -5.0469e-02,  8.0893e-03,\n",
      "          4.8704e-03,  1.7973e-02,  1.0108e-01, -4.5268e-02, -3.8983e-02,\n",
      "         -6.2415e-02,  1.0759e-01, -3.2837e-02, -6.8561e-02, -9.7366e-02,\n",
      "          1.2368e-02, -8.9979e-02,  9.1153e-02,  2.4158e-02,  8.6663e-02,\n",
      "          9.0132e-02,  9.3734e-02, -7.4002e-02, -9.0223e-02,  7.6774e-02,\n",
      "          6.3596e-02,  4.6887e-02, -8.3879e-02, -2.0575e-02, -5.2755e-03,\n",
      "         -4.0727e-02, -8.2409e-02,  9.0230e-02, -9.5811e-02,  3.4441e-03,\n",
      "         -4.0208e-02,  6.1549e-02,  3.4055e-02,  9.0756e-02,  4.4617e-02,\n",
      "         -7.9246e-02,  8.0965e-02,  2.7419e-02,  8.6576e-02, -7.9812e-02,\n",
      "          5.9625e-02,  3.3152e-02, -9.2108e-02, -5.5823e-02, -9.6413e-02,\n",
      "         -9.1236e-02,  3.2771e-02, -3.8434e-02,  4.6908e-02,  6.9696e-02,\n",
      "          2.7985e-03,  7.7836e-03,  8.8102e-02, -8.8428e-02, -1.0716e-01,\n",
      "          3.6613e-02, -3.6071e-02, -7.0305e-02, -6.1536e-02,  6.3312e-02,\n",
      "          1.3959e-02, -8.0568e-02,  6.5229e-02,  1.1368e-02,  3.1178e-02,\n",
      "          4.1034e-02,  2.5743e-02,  4.9328e-02,  3.1372e-02,  1.9083e-02,\n",
      "         -7.4029e-03,  4.0904e-02,  2.6813e-02,  9.4049e-02, -1.0621e-01,\n",
      "          1.9187e-02,  9.2334e-02, -2.2998e-02,  4.1980e-03,  3.1522e-02,\n",
      "          1.0209e-01, -1.0262e-01, -2.3082e-02,  3.9045e-02,  1.0135e-01,\n",
      "         -5.9961e-02,  4.3928e-02,  1.8729e-02,  9.2089e-02,  7.3255e-02,\n",
      "         -5.7475e-02, -1.0406e-01, -3.1206e-02,  9.5330e-02,  1.0643e-01,\n",
      "         -7.5634e-02, -5.9979e-02,  3.7854e-02, -2.4688e-02, -9.5038e-02,\n",
      "         -8.8308e-02,  6.0602e-02, -6.0301e-02,  4.6142e-02,  4.1799e-02,\n",
      "          4.7902e-02, -6.4161e-02,  8.8896e-03, -1.0484e-01,  3.5341e-02,\n",
      "         -1.0701e-01, -3.7717e-02,  2.5533e-02,  7.2821e-02,  2.2764e-02,\n",
      "          4.0299e-02, -5.0536e-02, -6.7735e-02, -4.5872e-02, -5.3376e-02,\n",
      "         -6.7982e-02,  1.0909e-02, -6.9482e-02,  7.8946e-02, -6.2348e-02,\n",
      "         -7.7052e-02,  4.2645e-02,  8.4781e-02, -2.6800e-02, -8.7252e-02,\n",
      "          5.5613e-03,  1.7751e-02,  1.0456e-01, -6.0884e-02, -7.9088e-02,\n",
      "          1.0396e-01, -4.2767e-02,  9.9602e-02, -1.8826e-02, -8.3841e-02,\n",
      "         -1.4578e-02, -2.3003e-02, -2.4414e-02, -6.3080e-02, -3.7980e-02,\n",
      "         -8.4114e-02, -2.6510e-02,  1.0714e-01,  9.2956e-02, -7.5382e-02,\n",
      "         -7.1955e-02, -9.9416e-02,  1.0059e-01, -6.6939e-02, -1.3343e-02,\n",
      "         -9.2446e-02, -7.3426e-02,  3.4292e-02, -6.1926e-03, -1.0221e-01,\n",
      "         -7.2000e-02,  9.1751e-02, -2.0468e-02,  1.1471e-02,  9.1863e-02,\n",
      "          9.0514e-02, -1.0317e-02,  3.5814e-02, -1.0019e-01,  1.4552e-02,\n",
      "          3.8370e-02,  4.7075e-02,  3.1395e-02, -5.8571e-02, -1.0666e-01,\n",
      "          3.3000e-02,  1.7342e-02, -7.3838e-02, -1.0322e-01, -5.3685e-03,\n",
      "         -4.3290e-02, -1.0767e-01, -9.3536e-03, -5.0622e-02,  1.0555e-01,\n",
      "         -9.5153e-02, -6.9477e-02, -1.2355e-02, -7.0557e-02, -1.2388e-02,\n",
      "         -7.5594e-02, -6.4847e-02, -8.7803e-02, -7.6449e-02,  3.0314e-02,\n",
      "         -3.1117e-03,  4.6227e-02, -3.9543e-02, -3.2457e-02, -8.4600e-02,\n",
      "         -8.8802e-03,  4.0098e-02,  4.3195e-02,  9.3620e-02,  9.3825e-02,\n",
      "          8.3744e-03, -5.5640e-02, -9.8072e-02,  5.0510e-02,  1.5377e-02,\n",
      "          5.1516e-02, -3.2162e-02, -4.1641e-02, -9.6876e-02,  2.3241e-02,\n",
      "         -3.6276e-03, -6.8338e-02, -5.5738e-02, -7.2887e-02,  5.5277e-02,\n",
      "          1.3294e-02,  5.0750e-02, -3.7551e-02, -8.8432e-02,  1.0331e-01,\n",
      "          2.8633e-02,  2.6973e-02,  9.7633e-03,  8.9798e-02,  6.6499e-02,\n",
      "          6.1044e-02,  2.6019e-02,  7.7472e-02, -3.8097e-02,  8.7602e-03,\n",
      "          6.8182e-02,  5.7112e-02,  5.1098e-02,  8.6484e-02,  7.7295e-02,\n",
      "          4.3590e-02,  1.0787e-01, -1.0619e-01, -6.0892e-02, -1.5593e-02,\n",
      "         -1.0218e-01, -6.9343e-02,  7.9880e-03, -2.9443e-02,  1.8945e-02,\n",
      "         -8.8997e-02, -3.8422e-02, -8.5770e-02,  1.0300e-01,  5.6192e-02,\n",
      "         -4.6663e-02,  5.1501e-02, -2.5226e-03,  6.5033e-02,  6.7038e-02,\n",
      "         -4.1119e-02,  1.0192e-01,  4.6991e-04,  1.0546e-01,  1.9462e-02,\n",
      "          9.2060e-02,  5.4239e-02]], device='cuda:0')), ('bias', tensor([-0.0299], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "print(model.fc.state_dict())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 [train]: 100%|██████████| 30/30 [00:02<00:00, 11.93batch/s, batch_loss=0.197]\n",
      "Epoch 0 [val]: 100%|██████████| 4/4 [00:00<00:00, 16.57batch/s, batch_loss=0.69] \n",
      "Epoch 1 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.47batch/s, batch_loss=0.93] \n",
      "Epoch 1 [val]: 100%|██████████| 4/4 [00:00<00:00, 15.79batch/s, batch_loss=0.47] \n",
      "Epoch 2 [train]: 100%|██████████| 30/30 [00:02<00:00, 13.21batch/s, batch_loss=0.265]\n",
      "Epoch 2 [val]: 100%|██████████| 4/4 [00:00<00:00, 22.24batch/s, batch_loss=0.853]\n",
      "Epoch 3 [train]: 100%|██████████| 30/30 [00:02<00:00, 13.72batch/s, batch_loss=0.77] \n",
      "Epoch 3 [val]: 100%|██████████| 4/4 [00:00<00:00, 18.21batch/s, batch_loss=0.133]\n",
      "Epoch 4 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.99batch/s, batch_loss=0.0741]\n",
      "Epoch 4 [val]: 100%|██████████| 4/4 [00:00<00:00, 15.57batch/s, batch_loss=0.236]\n",
      "Epoch 5 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.79batch/s, batch_loss=0.391] \n",
      "Epoch 5 [val]: 100%|██████████| 4/4 [00:00<00:00, 18.67batch/s, batch_loss=0.278]\n",
      "Epoch 6 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.63batch/s, batch_loss=0.0206]\n",
      "Epoch 6 [val]: 100%|██████████| 4/4 [00:00<00:00, 17.45batch/s, batch_loss=0.265]\n",
      "Epoch 7 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.33batch/s, batch_loss=0.397] \n",
      "Epoch 7 [val]: 100%|██████████| 4/4 [00:00<00:00, 21.08batch/s, batch_loss=0.306]\n",
      "Epoch 8 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.38batch/s, batch_loss=0.0684]\n",
      "Epoch 8 [val]: 100%|██████████| 4/4 [00:00<00:00, 17.48batch/s, batch_loss=0.0729]\n",
      "Epoch 9 [train]: 100%|██████████| 30/30 [00:02<00:00, 13.40batch/s, batch_loss=5.58]  \n",
      "Epoch 9 [val]: 100%|██████████| 4/4 [00:00<00:00, 18.11batch/s, batch_loss=0.395]\n",
      "Epoch 10 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.56batch/s, batch_loss=0.407] \n",
      "Epoch 10 [val]: 100%|██████████| 4/4 [00:00<00:00, 16.64batch/s, batch_loss=0.973]\n",
      "Epoch 11 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.90batch/s, batch_loss=0.91]  \n",
      "Epoch 11 [val]: 100%|██████████| 4/4 [00:00<00:00, 17.72batch/s, batch_loss=0.12] \n",
      "Epoch 12 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.95batch/s, batch_loss=0.344] \n",
      "Epoch 12 [val]: 100%|██████████| 4/4 [00:00<00:00, 18.89batch/s, batch_loss=0.207]\n",
      "Epoch 13 [train]: 100%|██████████| 30/30 [00:02<00:00, 13.00batch/s, batch_loss=0.315] \n",
      "Epoch 13 [val]: 100%|██████████| 4/4 [00:00<00:00, 18.28batch/s, batch_loss=1.3]  \n",
      "Epoch 14 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.20batch/s, batch_loss=0.79]  \n",
      "Epoch 14 [val]: 100%|██████████| 4/4 [00:00<00:00, 17.64batch/s, batch_loss=0.447]\n",
      "Epoch 15 [train]: 100%|██████████| 30/30 [00:02<00:00, 14.14batch/s, batch_loss=0.105] \n",
      "Epoch 15 [val]: 100%|██████████| 4/4 [00:00<00:00, 16.64batch/s, batch_loss=0.17] \n",
      "Epoch 16 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.56batch/s, batch_loss=0.0981]\n",
      "Epoch 16 [val]: 100%|██████████| 4/4 [00:00<00:00, 17.83batch/s, batch_loss=0.219] \n",
      "Epoch 17 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.53batch/s, batch_loss=0.203] \n",
      "Epoch 17 [val]: 100%|██████████| 4/4 [00:00<00:00, 15.54batch/s, batch_loss=0.203]\n",
      "Epoch 18 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.75batch/s, batch_loss=0.468] \n",
      "Epoch 18 [val]: 100%|██████████| 4/4 [00:00<00:00, 15.64batch/s, batch_loss=0.474]\n",
      "Epoch 19 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.59batch/s, batch_loss=0.149] \n",
      "Epoch 19 [val]: 100%|██████████| 4/4 [00:00<00:00, 18.71batch/s, batch_loss=0.187]\n",
      "Epoch 20 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.63batch/s, batch_loss=0.0751]\n",
      "Epoch 20 [val]: 100%|██████████| 4/4 [00:00<00:00, 17.54batch/s, batch_loss=0.243]\n",
      "Epoch 21 [train]: 100%|██████████| 30/30 [00:02<00:00, 13.22batch/s, batch_loss=0.0193]\n",
      "Epoch 21 [val]: 100%|██████████| 4/4 [00:00<00:00, 15.02batch/s, batch_loss=0.187] \n",
      "Epoch 22 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.91batch/s, batch_loss=0.0411]\n",
      "Epoch 22 [val]: 100%|██████████| 4/4 [00:00<00:00, 16.86batch/s, batch_loss=0.0571]\n",
      "Epoch 23 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.05batch/s, batch_loss=0.416] \n",
      "Epoch 23 [val]: 100%|██████████| 4/4 [00:00<00:00, 18.34batch/s, batch_loss=0.597] \n",
      "Epoch 24 [train]: 100%|██████████| 30/30 [00:02<00:00, 13.17batch/s, batch_loss=0.511] \n",
      "Epoch 24 [val]: 100%|██████████| 4/4 [00:00<00:00, 13.57batch/s, batch_loss=0.158]\n",
      "Epoch 25 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.29batch/s, batch_loss=0.14]  \n",
      "Epoch 25 [val]: 100%|██████████| 4/4 [00:00<00:00, 19.37batch/s, batch_loss=0.0674]\n",
      "Epoch 26 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.89batch/s, batch_loss=0.348] \n",
      "Epoch 26 [val]: 100%|██████████| 4/4 [00:00<00:00, 18.58batch/s, batch_loss=0.378]\n",
      "Epoch 27 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.48batch/s, batch_loss=0.0255]\n",
      "Epoch 27 [val]: 100%|██████████| 4/4 [00:00<00:00, 19.54batch/s, batch_loss=0.59] \n",
      "Epoch 28 [train]: 100%|██████████| 30/30 [00:02<00:00, 13.40batch/s, batch_loss=0.401] \n",
      "Epoch 28 [val]: 100%|██████████| 4/4 [00:00<00:00, 19.03batch/s, batch_loss=0.256]\n",
      "Epoch 29 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.37batch/s, batch_loss=0.109] \n",
      "Epoch 29 [val]: 100%|██████████| 4/4 [00:00<00:00, 17.27batch/s, batch_loss=1.3] \n",
      "Epoch 30 [train]: 100%|██████████| 30/30 [00:02<00:00, 13.57batch/s, batch_loss=0.00625]\n",
      "Epoch 30 [val]: 100%|██████████| 4/4 [00:00<00:00, 18.52batch/s, batch_loss=0.271]\n",
      "Epoch 31 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.89batch/s, batch_loss=1.24]  \n",
      "Epoch 31 [val]: 100%|██████████| 4/4 [00:00<00:00, 17.36batch/s, batch_loss=0.064] \n",
      "Epoch 32 [train]: 100%|██████████| 30/30 [00:02<00:00, 14.40batch/s, batch_loss=0.388] \n",
      "Epoch 32 [val]: 100%|██████████| 4/4 [00:00<00:00, 14.81batch/s, batch_loss=0.992]\n",
      "Epoch 33 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.09batch/s, batch_loss=0.0705]\n",
      "Epoch 33 [val]: 100%|██████████| 4/4 [00:00<00:00, 15.31batch/s, batch_loss=0.603]\n",
      "Epoch 34 [train]: 100%|██████████| 30/30 [00:02<00:00, 13.41batch/s, batch_loss=0.356] \n",
      "Epoch 34 [val]: 100%|██████████| 4/4 [00:00<00:00, 18.86batch/s, batch_loss=0.142]\n",
      "Epoch 35 [train]: 100%|██████████| 30/30 [00:02<00:00, 13.01batch/s, batch_loss=0.0463]\n",
      "Epoch 35 [val]: 100%|██████████| 4/4 [00:00<00:00, 15.60batch/s, batch_loss=0.31] \n",
      "Epoch 36 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.22batch/s, batch_loss=0.245] \n",
      "Epoch 36 [val]: 100%|██████████| 4/4 [00:00<00:00, 17.03batch/s, batch_loss=0.287] \n",
      "Epoch 37 [train]: 100%|██████████| 30/30 [00:02<00:00, 11.78batch/s, batch_loss=0.189] \n",
      "Epoch 37 [val]: 100%|██████████| 4/4 [00:00<00:00, 15.73batch/s, batch_loss=0.107] \n",
      "Epoch 38 [train]: 100%|██████████| 30/30 [00:02<00:00, 12.80batch/s, batch_loss=0.69]  \n",
      "Epoch 38 [val]: 100%|██████████| 4/4 [00:00<00:00, 19.14batch/s, batch_loss=0.313]\n",
      "Epoch 39 [train]: 100%|██████████| 30/30 [00:02<00:00, 13.01batch/s, batch_loss=0.0047]\n",
      "Epoch 39 [val]: 100%|██████████| 4/4 [00:00<00:00, 19.18batch/s, batch_loss=0.186]\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Get all the parameters excepts the weights and bias of fc layer\n",
    "base_params = [param for name, param in model.named_parameters()\n",
    "               if name not in [\"fc.weight\", \"fc.bias\"]]\n",
    "\n",
    "# Define the optimizer, with a greater learning rate for the new fc layer\n",
    "optimizer = optim.SGD([\n",
    "    {\"params\": base_params},\n",
    "    {\"params\": model.fc.parameters(), \"lr\": 1e-3},\n",
    "],\n",
    "    lr=1e-4, momentum=0.9, weight_decay=0.001)\n",
    "\n",
    "# An epoch is one complete pass of the training dataset through the network\n",
    "NB_EPOCHS = 40\n",
    "\n",
    "# Loop over the epochs\n",
    "for epoch in range(NB_EPOCHS):\n",
    "    \n",
    "    # Training\n",
    "    train_loss = 0.\n",
    "    \n",
    "    # Configure the model for training\n",
    "    # (good practice, only necessary if the model operates differently for\n",
    "    # training and validation)\n",
    "    model.train()\n",
    "    \n",
    "    # Add a progress bar\n",
    "    train_loader_pbar = tqdm(train_loader, unit=\"batch\")\n",
    "    \n",
    "    # Loop over the training batches\n",
    "    for images, traversal_scores in train_loader_pbar:\n",
    "        \n",
    "        # Print the epoch and training mode\n",
    "        train_loader_pbar.set_description(f\"Epoch {epoch} [train]\")\n",
    "        \n",
    "        # Move images and traversal scores to GPU (if available)\n",
    "        images = images.to(device)\n",
    "        traversal_scores = traversal_scores.to(device)\n",
    "        \n",
    "        # Zero out gradients before each backpropagation pass, to avoid that\n",
    "        # they accumulate\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform forward pass\n",
    "        predicted_traversal_scores = model(images)\n",
    "        \n",
    "        # Compute loss \n",
    "        loss = criterion(predicted_traversal_scores, traversal_scores)\n",
    "        \n",
    "        # Print the batch loss next to the progress bar\n",
    "        train_loader_pbar.set_postfix(batch_loss=loss.item())\n",
    "        \n",
    "        # Perform backpropagation (compute gradients)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Adjust parameters based on gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate batch loss to average over the epoch\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    # Validation\n",
    "    val_loss = 0.\n",
    "    \n",
    "    # Configure the model for testing\n",
    "    model.eval()\n",
    "    \n",
    "    # Add a progress bar\n",
    "    val_loader_pbar = tqdm(val_loader, unit=\"batch\")\n",
    "    \n",
    "    # Loop over the validation batches\n",
    "    for images, traversal_scores in val_loader_pbar:\n",
    "        \n",
    "        # Print the epoch and validation mode\n",
    "        val_loader_pbar.set_description(f\"Epoch {epoch} [val]\")\n",
    "        \n",
    "        # Move images and traversal scores to GPU (if available)\n",
    "        images = images.to(device)\n",
    "        traversal_scores = traversal_scores.to(device)\n",
    "        \n",
    "        # Perform forward pass (only, no backpropagation)\n",
    "        predicted_traversal_scores = model(images)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(predicted_traversal_scores, traversal_scores)\n",
    "        \n",
    "        # Print the batch loss next to the progress bar\n",
    "        val_loader_pbar.set_postfix(batch_loss=loss.item())\n",
    "        \n",
    "        # Accumulate batch loss to average over the epoch\n",
    "        val_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    # # Display the computed losses\n",
    "    # print(f\"Epoch {epoch}: Train Loss: {train_loss/len(train_loader)}\\\n",
    "    #       Validation Loss: {val_loss/len(val_loader)}\")\n",
    "    # loss_values[0, epoch] = train_loss/len(train_loader)\n",
    "    # loss_values[1, epoch] = val_loss/len(val_loader)\n",
    "    \n",
    "    # Add the losses to TensorBoard\n",
    "    tensorboard.add_scalar(\"train_loss\", train_loss/len(train_loader), epoch)\n",
    "    tensorboard.add_scalar(\"val_loss\", val_loss/len(val_loader), epoch)\n",
    "\n",
    "# Close TensorBoard\n",
    "tensorboard.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.20328552275896072\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_loss = 0.\n",
    "\n",
    "denormalize = transforms.Compose([\n",
    "    transforms.Normalize(\n",
    "        mean=[0., 0., 0.],\n",
    "        std=[1/0.229, 1/0.224, 1/0.225]\n",
    "    ),\n",
    "    transforms.Normalize(\n",
    "        mean=[-0.485, -0.456, -0.406],\n",
    "        std=[1., 1., 1.]\n",
    "    ),\n",
    "    transforms.ToPILImage(),\n",
    "])\n",
    "\n",
    "# Loop over the testing batches\n",
    "for images, traversal_scores in test_loader:\n",
    "    \n",
    "    images = images.to(device)\n",
    "    traversal_scores = traversal_scores.to(device)\n",
    "    \n",
    "    # Perform forward pass\n",
    "    predicted_traversal_scores = model(images)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(predicted_traversal_scores, traversal_scores)\n",
    "    \n",
    "    # Accumulate batch loss to average of the entire testing set\n",
    "    test_loss += loss.item()\n",
    "    # print(loss.item())\n",
    "    \n",
    "    # for i in range(images.shape[0]):\n",
    "    #     plt.imshow(denormalize(images[i]))\n",
    "\n",
    "print(f\"Test loss: {test_loss/len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      " tensor([[-0.2548],\n",
      "        [ 0.3232],\n",
      "        [ 0.1772],\n",
      "        [-0.5162],\n",
      "        [-0.2846],\n",
      "        [ 0.7074],\n",
      "        [-1.0384],\n",
      "        [ 1.9674],\n",
      "        [ 0.0108],\n",
      "        [ 0.2014],\n",
      "        [ 0.0790],\n",
      "        [-0.3786],\n",
      "        [ 0.4753],\n",
      "        [-0.1176],\n",
      "        [ 0.3550],\n",
      "        [-0.9295]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Ground truth:\n",
      " tensor([[ 0.3576],\n",
      "        [ 0.1939],\n",
      "        [ 0.4067],\n",
      "        [-0.4216],\n",
      "        [-0.5185],\n",
      "        [ 1.9260],\n",
      "        [-0.6316],\n",
      "        [ 2.2495],\n",
      "        [ 0.4743],\n",
      "        [ 0.3830],\n",
      "        [ 0.7960],\n",
      "        [-0.4424],\n",
      "        [ 0.8373],\n",
      "        [ 0.1509],\n",
      "        [ 0.3877],\n",
      "        [-0.7865]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "images, traversal_scores = next(iter(test_loader))\n",
    "\n",
    "images = images.to(device)\n",
    "traversal_scores = traversal_scores.to(device)\n",
    "\n",
    "predicted_traversal_scores = model(images)\n",
    "\n",
    "print(\"Prediction:\\n\", predicted_traversal_scores)\n",
    "print(\"Ground truth:\\n\", traversal_scores)\n",
    "\n",
    "# print(predicted_traversal_scores-traversal_scores)\n",
    "\n",
    "# predicted_traversal_scores = predicted_traversal_scores.to(\"cpu\").detach().numpy()\n",
    "# plt.hist(predicted_traversal_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 8., 15.,  5., 21.,  5.,  2.,  1.,  0.,  0.,  1.]),\n",
       " array([-3.947405 , -2.382649 , -0.8178927,  0.7468636,  2.3116198,\n",
       "         3.8763762,  5.441132 ,  7.0058885,  8.570644 , 10.135401 ,\n",
       "        11.700157 ], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traversal_scores_train = []\n",
    "\n",
    "for _, score in val_set:\n",
    "    traversal_scores_train.append(score[0])\n",
    "    \n",
    "# print(traversal_scores_train)\n",
    "plt.hist(traversal_scores_train, bins=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model parameters\n",
    "torch.save(model.state_dict(), \"resnet18_fine_tuned_path_grass_pca_robust.params\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
