{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "# A module to print a model summary (outputs shape, number of parameters, ...)\n",
    "import torchsummary\n",
    "\n",
    "# TensorBoard for visualization\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data to be used\n",
    "DATASET = \"../../datasets/dataset_all/\"\n",
    "\n",
    "\n",
    "class TraversabilityDataset(Dataset):\n",
    "    \"\"\"Custom Dataset class to represent our dataset\n",
    "    It includes data and information about the data\n",
    "\n",
    "    Args:\n",
    "        Dataset (class): Abstract class which represents a dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, traversal_costs_file, images_directory,\n",
    "                 transform=None):\n",
    "        \"\"\"Constructor of the class\n",
    "\n",
    "        Args:\n",
    "            traversal_costs_file (string): Path to the csv file which contains\n",
    "            images index and their associated traversal cost\n",
    "            images_directory (string): Directory with all the images\n",
    "            transform (callable, optional): Transforms to be applied on a\n",
    "            sample. Defaults to None.\n",
    "        \"\"\"\n",
    "        # Read the csv file\n",
    "        self.traversal_costs_frame = pd.read_csv(traversal_costs_file)\n",
    "        \n",
    "        # Initialize the name of the images directory\n",
    "        self.images_directory = images_directory\n",
    "        \n",
    "        # Initialize the transforms\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the dataset\n",
    "\n",
    "        Returns:\n",
    "            int: Number of samples\n",
    "        \"\"\"\n",
    "        return len(self.traversal_costs_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Allow to access a sample by its index\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of a sample\n",
    "\n",
    "        Returns:\n",
    "            list: Sample at index idx\n",
    "            ([image, traversal_cost])\n",
    "        \"\"\"\n",
    "        # Get the image name at index idx\n",
    "        image_name = os.path.join(self.images_directory,\n",
    "                                  self.traversal_costs_frame.iloc[idx, 0])\n",
    "        \n",
    "        # Read the image\n",
    "        image = Image.open(image_name)\n",
    "        \n",
    "        # Get the corresponding traversal cost\n",
    "        traversal_cost = self.traversal_costs_frame.iloc[idx, 1:]\n",
    "        traversal_cost = np.array(traversal_cost)\n",
    "        traversal_cost = 100*np.float32(traversal_cost)\n",
    "\n",
    "        # Create the sample\n",
    "        sample = [image, traversal_cost]\n",
    "\n",
    "        # Eventually apply transforms to the image\n",
    "        if self.transform:\n",
    "            sample[0] = self.transform(sample[0])\n",
    "\n",
    "        return sample\n",
    " \n",
    "\n",
    "# Compose several transforms together to be applied to training data\n",
    "# (Note that transforms are not applied yet)\n",
    "train_transform = transforms.Compose([\n",
    "    # Reduce the size of the images\n",
    "    # (if size is an int, the smaller edge of the\n",
    "    # image will be matched to this number and the ration is kept)\n",
    "    transforms.Resize(100),\n",
    "    \n",
    "    # Crop the image at the center\n",
    "    # (if size is an int, a square crop is made)\n",
    "    # transforms.CenterCrop(100),\n",
    "    \n",
    "    # Crop a random square in the image\n",
    "    transforms.RandomCrop(100),\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    # transforms.Grayscale(num_output_channels=1),\n",
    "    \n",
    "    # Perform horizontal flip of the image with a probability of 0.5\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    \n",
    "    # Convert a PIL Image or numpy.ndarray to tensor\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Normalize a tensor image with pre-computed mean and standard deviation\n",
    "    # (based on the data used to train the model(s))\n",
    "    # (be careful, it only works on torch.*Tensor)\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Define a different set of transforms testing\n",
    "# (for instance we do not need to flip the image)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(100),\n",
    "    # transforms.Grayscale(),\n",
    "    transforms.CenterCrop(100),\n",
    "    # transforms.RandomCrop(100),\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Mean and standard deviation were pre-computed on the training data\n",
    "    # (on the ImageNet dataset)\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "\n",
    "# Create a Dataset instance for our training data\n",
    "data = TraversabilityDataset(\n",
    "    traversal_costs_file=DATASET+\"traversal_costs.csv\",\n",
    "    images_directory=DATASET+\"zed_node_rgb_image_rect_color\",\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "# Split our training dataset into a training dataset and a validation dataset\n",
    "train_set, val_set, test_set = random_split(data, [0.8, 0.1, 0.1])\n",
    "\n",
    "\n",
    "# Combine a dataset and a sampler, and provide an iterable over the dataset\n",
    "# (setting shuffle argument to true calls a RandomSampler, and avoids to\n",
    "# have to create a Sampler object)\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_size=16,\n",
    "    shuffle=False,  # SequentialSampler\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO: why is not the GPU available?\n",
    "# Use a GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model design and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0366, -0.0329,  0.0907, -0.0469, -0.0030, -0.0895,  0.0767, -0.0836,\n",
       "         -0.0575, -0.0970,  0.0304, -0.0112, -0.0604,  0.0215,  0.0900, -0.1034,\n",
       "          0.0777,  0.0037,  0.0920, -0.0243,  0.0055,  0.0276,  0.0746, -0.0627,\n",
       "         -0.0969,  0.0719, -0.0351,  0.0144, -0.0052,  0.1031,  0.0780,  0.0118,\n",
       "         -0.0451, -0.0513,  0.0944, -0.0811,  0.0059, -0.0368, -0.0081,  0.0835,\n",
       "         -0.0071,  0.0809, -0.0777, -0.0984,  0.0285,  0.0352,  0.0300, -0.0046,\n",
       "         -0.0489, -0.0595,  0.0583, -0.0575, -0.0143, -0.0475, -0.0903,  0.0313,\n",
       "          0.0198, -0.0491, -0.0375, -0.0887,  0.0690,  0.0547,  0.0510, -0.0749,\n",
       "          0.0459,  0.0162,  0.0301, -0.0493, -0.0940,  0.0012,  0.0889, -0.0310,\n",
       "          0.0205,  0.0958, -0.0272,  0.0537,  0.0476,  0.1027, -0.0609, -0.0811,\n",
       "          0.0529, -0.0010,  0.0190,  0.0778,  0.0166, -0.0555,  0.0981,  0.0268,\n",
       "          0.0128, -0.0903, -0.0975, -0.0945,  0.0506, -0.0063, -0.1080,  0.0058,\n",
       "         -0.0790,  0.0632, -0.0960, -0.0946, -0.0121, -0.0707, -0.0913, -0.0280,\n",
       "         -0.0502, -0.0276,  0.0504,  0.0401, -0.0341, -0.0168,  0.0255, -0.0990,\n",
       "         -0.0067,  0.0604,  0.0527, -0.0133,  0.0027, -0.0367, -0.0029,  0.0988,\n",
       "         -0.0857,  0.0795,  0.0582,  0.0650,  0.0057, -0.1053, -0.0390, -0.0273,\n",
       "          0.0503,  0.0829, -0.0946,  0.0937,  0.0541, -0.0686, -0.0788, -0.0090,\n",
       "          0.0133, -0.0704, -0.0851, -0.0362, -0.0163,  0.0444,  0.0616,  0.0400,\n",
       "          0.0510, -0.0979, -0.0764, -0.0853,  0.0551,  0.0477,  0.1053, -0.1038,\n",
       "          0.0795, -0.0035,  0.0394,  0.0216, -0.0882, -0.0186,  0.0210, -0.0796,\n",
       "          0.0099,  0.0726, -0.0362, -0.0858, -0.0846, -0.0310, -0.0266,  0.0540,\n",
       "          0.0746, -0.0879,  0.0434,  0.0353,  0.0284,  0.0460, -0.0327,  0.0398,\n",
       "          0.0424,  0.0297, -0.0577,  0.0047,  0.0344,  0.0784,  0.0607, -0.0407,\n",
       "          0.0998,  0.0872,  0.0730,  0.0097, -0.0870,  0.0552, -0.0647, -0.0751,\n",
       "          0.0132,  0.0368, -0.0098,  0.0075, -0.0237,  0.0085, -0.0935, -0.0424,\n",
       "          0.0434, -0.0742,  0.0173,  0.0874, -0.0167, -0.0991,  0.0533,  0.0336,\n",
       "          0.0976, -0.0969,  0.0208, -0.0990, -0.0109,  0.1010, -0.0452,  0.0684,\n",
       "         -0.0903,  0.0121,  0.0984, -0.0882,  0.0712, -0.0873,  0.0164,  0.1071,\n",
       "         -0.0986,  0.0057,  0.0415,  0.0433, -0.0817,  0.0025, -0.0337,  0.0433,\n",
       "          0.0321, -0.0917, -0.0710,  0.0248, -0.0439, -0.0910,  0.0163,  0.0357,\n",
       "          0.0650, -0.0987, -0.0085,  0.0803,  0.0289, -0.0018,  0.0999,  0.0608,\n",
       "          0.0979,  0.0721,  0.0754, -0.0054, -0.0835, -0.1076, -0.0256,  0.0599,\n",
       "          0.0415,  0.0986, -0.0209,  0.0201,  0.0564, -0.0035,  0.0454,  0.0613,\n",
       "          0.0110, -0.0843, -0.0624, -0.0968, -0.0555, -0.0537,  0.1005,  0.0508,\n",
       "         -0.0433,  0.0875, -0.0310, -0.1052,  0.0440,  0.0163,  0.0304, -0.1066,\n",
       "         -0.0086,  0.0630,  0.0160, -0.0587,  0.0450, -0.0417,  0.0329,  0.0135,\n",
       "         -0.0670,  0.0409,  0.0547,  0.0109, -0.1080,  0.0374, -0.0327,  0.0804,\n",
       "         -0.1052, -0.0332,  0.0192, -0.0669, -0.0815, -0.0721,  0.0751,  0.0708,\n",
       "         -0.0458,  0.0592, -0.0186,  0.0910, -0.0456, -0.0316, -0.0825,  0.0714,\n",
       "         -0.0739,  0.0301,  0.0860, -0.0951,  0.0724,  0.1006, -0.0403, -0.0366,\n",
       "         -0.0516,  0.0489, -0.0409,  0.0154,  0.0931, -0.0426,  0.0410,  0.0284,\n",
       "         -0.0783, -0.0047, -0.0294, -0.0358,  0.0977, -0.0068,  0.0498,  0.0658,\n",
       "         -0.0124,  0.0055, -0.0220,  0.0161,  0.0280, -0.0978, -0.0192,  0.0602,\n",
       "          0.1060, -0.0963, -0.0967, -0.1027, -0.0762, -0.0331,  0.0565, -0.0879,\n",
       "         -0.0110, -0.0106, -0.0747,  0.0011, -0.0191, -0.0592,  0.0263,  0.0066,\n",
       "          0.0591,  0.0394,  0.0687,  0.0412,  0.0610,  0.0303, -0.0033, -0.0716,\n",
       "          0.1064,  0.1047, -0.0834, -0.0373,  0.0827,  0.0617, -0.0186,  0.0943,\n",
       "          0.1076,  0.0494, -0.0286,  0.0172,  0.0210,  0.0856,  0.1019,  0.0398,\n",
       "         -0.0771, -0.0813, -0.0203,  0.0686,  0.0005, -0.0601, -0.0711,  0.0138,\n",
       "         -0.0839, -0.0369,  0.0676, -0.1079,  0.0896, -0.0976, -0.0711,  0.0549,\n",
       "         -0.0427,  0.0739, -0.1018,  0.0957,  0.0314,  0.0487,  0.0655,  0.0947,\n",
       "          0.0173, -0.0019,  0.0908, -0.0787,  0.0120, -0.0333, -0.1056,  0.0424,\n",
       "         -0.0874, -0.0560,  0.0341,  0.1051,  0.0010,  0.0085, -0.0878, -0.0721,\n",
       "         -0.0137, -0.0749,  0.0309, -0.0131, -0.0195, -0.0133, -0.0887,  0.0464,\n",
       "          0.0761,  0.0073,  0.0061,  0.0065,  0.0818,  0.0620, -0.0581,  0.0781,\n",
       "          0.1042, -0.0232, -0.0901,  0.0120,  0.0639,  0.0017,  0.0258, -0.0618,\n",
       "         -0.0731, -0.0982, -0.0728,  0.0909,  0.0967, -0.1008, -0.0186, -0.0205,\n",
       "          0.0384, -0.0630, -0.0033, -0.0564, -0.0259,  0.0652, -0.0060,  0.0123,\n",
       "          0.0110, -0.0385,  0.0890, -0.0065,  0.0226,  0.0354, -0.0553,  0.0399,\n",
       "          0.1003,  0.0780, -0.0458,  0.0062,  0.0423, -0.0959, -0.0612, -0.0893,\n",
       "         -0.0677, -0.0226, -0.0023,  0.0594,  0.1021,  0.0568,  0.0591,  0.0880,\n",
       "          0.0495, -0.0839, -0.0787, -0.0088, -0.0055, -0.0468, -0.0905,  0.0747,\n",
       "         -0.0694,  0.0930,  0.0256,  0.0709, -0.1044,  0.0437,  0.0386, -0.0751,\n",
       "         -0.0062, -0.0960, -0.0495,  0.0077,  0.0544, -0.0870,  0.0242,  0.0070]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open TensorBoard\n",
    "tensorboard = SummaryWriter()\n",
    "\n",
    "# Load the pre-trained AlexNet model\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT).to(device=device)\n",
    "\n",
    "\n",
    "\n",
    "# Replace the last layer by a fully-connected one with 1 output\n",
    "model.fc = nn.Linear(model.fc.in_features, 1, device=device)\n",
    "\n",
    "# print(next(model.fc.parameters()).device)\n",
    "# print(next(model.parameters()).device)\n",
    "\n",
    "# Display the architecture in TensorBoard\n",
    "images, traversal_scores = next(iter(train_loader))\n",
    "images = images.to(device)\n",
    "# print(images.device)\n",
    "tensorboard.add_graph(model, images)\n",
    "\n",
    "# print(model)\n",
    "# print(torchsummary.summary(model, (3, 100, 100)))\n",
    "\n",
    "# Initialize the last layer using Xavier initialization\n",
    "nn.init.xavier_uniform_(model.fc.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conv1.weight', torch.Size([64, 3, 7, 7])),\n",
       " ('bn1.weight', torch.Size([64])),\n",
       " ('bn1.bias', torch.Size([64])),\n",
       " ('layer1.0.conv1.weight', torch.Size([64, 64, 3, 3])),\n",
       " ('layer1.0.bn1.weight', torch.Size([64])),\n",
       " ('layer1.0.bn1.bias', torch.Size([64])),\n",
       " ('layer1.0.conv2.weight', torch.Size([64, 64, 3, 3])),\n",
       " ('layer1.0.bn2.weight', torch.Size([64])),\n",
       " ('layer1.0.bn2.bias', torch.Size([64])),\n",
       " ('layer1.1.conv1.weight', torch.Size([64, 64, 3, 3])),\n",
       " ('layer1.1.bn1.weight', torch.Size([64])),\n",
       " ('layer1.1.bn1.bias', torch.Size([64])),\n",
       " ('layer1.1.conv2.weight', torch.Size([64, 64, 3, 3])),\n",
       " ('layer1.1.bn2.weight', torch.Size([64])),\n",
       " ('layer1.1.bn2.bias', torch.Size([64])),\n",
       " ('layer2.0.conv1.weight', torch.Size([128, 64, 3, 3])),\n",
       " ('layer2.0.bn1.weight', torch.Size([128])),\n",
       " ('layer2.0.bn1.bias', torch.Size([128])),\n",
       " ('layer2.0.conv2.weight', torch.Size([128, 128, 3, 3])),\n",
       " ('layer2.0.bn2.weight', torch.Size([128])),\n",
       " ('layer2.0.bn2.bias', torch.Size([128])),\n",
       " ('layer2.0.downsample.0.weight', torch.Size([128, 64, 1, 1])),\n",
       " ('layer2.0.downsample.1.weight', torch.Size([128])),\n",
       " ('layer2.0.downsample.1.bias', torch.Size([128])),\n",
       " ('layer2.1.conv1.weight', torch.Size([128, 128, 3, 3])),\n",
       " ('layer2.1.bn1.weight', torch.Size([128])),\n",
       " ('layer2.1.bn1.bias', torch.Size([128])),\n",
       " ('layer2.1.conv2.weight', torch.Size([128, 128, 3, 3])),\n",
       " ('layer2.1.bn2.weight', torch.Size([128])),\n",
       " ('layer2.1.bn2.bias', torch.Size([128])),\n",
       " ('layer3.0.conv1.weight', torch.Size([256, 128, 3, 3])),\n",
       " ('layer3.0.bn1.weight', torch.Size([256])),\n",
       " ('layer3.0.bn1.bias', torch.Size([256])),\n",
       " ('layer3.0.conv2.weight', torch.Size([256, 256, 3, 3])),\n",
       " ('layer3.0.bn2.weight', torch.Size([256])),\n",
       " ('layer3.0.bn2.bias', torch.Size([256])),\n",
       " ('layer3.0.downsample.0.weight', torch.Size([256, 128, 1, 1])),\n",
       " ('layer3.0.downsample.1.weight', torch.Size([256])),\n",
       " ('layer3.0.downsample.1.bias', torch.Size([256])),\n",
       " ('layer3.1.conv1.weight', torch.Size([256, 256, 3, 3])),\n",
       " ('layer3.1.bn1.weight', torch.Size([256])),\n",
       " ('layer3.1.bn1.bias', torch.Size([256])),\n",
       " ('layer3.1.conv2.weight', torch.Size([256, 256, 3, 3])),\n",
       " ('layer3.1.bn2.weight', torch.Size([256])),\n",
       " ('layer3.1.bn2.bias', torch.Size([256])),\n",
       " ('layer4.0.conv1.weight', torch.Size([512, 256, 3, 3])),\n",
       " ('layer4.0.bn1.weight', torch.Size([512])),\n",
       " ('layer4.0.bn1.bias', torch.Size([512])),\n",
       " ('layer4.0.conv2.weight', torch.Size([512, 512, 3, 3])),\n",
       " ('layer4.0.bn2.weight', torch.Size([512])),\n",
       " ('layer4.0.bn2.bias', torch.Size([512])),\n",
       " ('layer4.0.downsample.0.weight', torch.Size([512, 256, 1, 1])),\n",
       " ('layer4.0.downsample.1.weight', torch.Size([512])),\n",
       " ('layer4.0.downsample.1.bias', torch.Size([512])),\n",
       " ('layer4.1.conv1.weight', torch.Size([512, 512, 3, 3])),\n",
       " ('layer4.1.bn1.weight', torch.Size([512])),\n",
       " ('layer4.1.bn1.bias', torch.Size([512])),\n",
       " ('layer4.1.conv2.weight', torch.Size([512, 512, 3, 3])),\n",
       " ('layer4.1.bn2.weight', torch.Size([512])),\n",
       " ('layer4.1.bn2.bias', torch.Size([512])),\n",
       " ('fc.weight', torch.Size([1, 512])),\n",
       " ('fc.bias', torch.Size([1]))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(name, param.shape) for name, param in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-7.7058e-03,  3.3215e-02, -1.0313e-01, -3.1313e-02,  3.3181e-02,\n",
      "          1.0164e-01, -9.5729e-02,  8.9491e-02, -9.4631e-02, -1.4007e-02,\n",
      "          6.0789e-03, -2.8962e-04,  6.7310e-02,  4.6872e-02, -1.3610e-02,\n",
      "         -7.4497e-02,  8.0381e-02, -1.5875e-02, -6.7730e-02,  2.5678e-02,\n",
      "          6.7734e-02,  5.9627e-02,  1.0749e-01, -7.8350e-02, -8.4878e-02,\n",
      "          4.7419e-02,  7.9102e-02,  4.7449e-02,  8.2736e-02,  6.9326e-02,\n",
      "          5.2080e-02, -1.0240e-01,  5.5825e-02,  3.9308e-02,  2.3794e-02,\n",
      "          9.7285e-02,  5.1489e-02,  2.9523e-02,  9.7410e-02,  3.6763e-02,\n",
      "          8.8270e-02, -8.0063e-02,  3.9146e-02, -1.7124e-02, -3.0918e-02,\n",
      "          1.2801e-02, -8.1253e-02,  1.8603e-02, -9.8077e-02,  1.0027e-01,\n",
      "         -8.6213e-02, -7.3725e-02,  1.0475e-01, -1.9780e-02, -6.7918e-02,\n",
      "          7.0182e-02, -8.6331e-02,  9.5562e-02,  9.3305e-02, -7.9027e-02,\n",
      "         -8.9907e-02, -3.8583e-02,  8.8826e-02,  3.0227e-02, -8.8472e-03,\n",
      "          5.2862e-03, -3.6672e-02, -8.6079e-02, -3.0019e-02, -7.4985e-02,\n",
      "          1.0792e-01, -5.7672e-03, -4.6688e-02, -9.6462e-03, -8.4370e-02,\n",
      "         -2.3013e-02, -1.6154e-02, -8.2872e-02, -9.4585e-02, -1.0083e-01,\n",
      "          8.2167e-02,  2.1440e-02, -8.5174e-02, -1.3931e-02, -9.2271e-02,\n",
      "         -2.1849e-02,  3.0072e-02,  4.4952e-02, -4.9038e-02,  1.0035e-01,\n",
      "         -2.5173e-02, -4.2883e-02,  6.3646e-02, -1.0380e-01,  9.8200e-02,\n",
      "          4.5325e-02, -4.2109e-02, -9.3420e-02, -6.1914e-02,  4.1269e-02,\n",
      "         -1.0738e-01,  3.1597e-02, -5.7601e-02,  5.7135e-02, -4.4681e-02,\n",
      "          4.2161e-02, -4.0350e-02,  6.8807e-02, -5.5635e-02,  1.0629e-01,\n",
      "         -1.4641e-02,  4.1478e-02,  4.5082e-02, -4.3043e-02, -2.9332e-02,\n",
      "         -5.0299e-02,  3.1890e-02, -6.4427e-03,  9.1711e-02,  3.4721e-02,\n",
      "         -8.7652e-02, -3.0281e-03,  1.5641e-02,  2.7762e-02,  3.6678e-02,\n",
      "          2.5629e-02, -6.8072e-02,  9.8360e-02, -1.0365e-01, -9.9955e-02,\n",
      "          9.5970e-03, -1.0526e-01,  5.7842e-02,  1.2113e-02,  9.7188e-02,\n",
      "          8.5155e-02,  8.4143e-02, -6.4733e-02,  4.2705e-04,  2.2575e-02,\n",
      "         -1.0432e-02, -1.0288e-01, -6.6768e-02, -7.1479e-02, -5.9553e-03,\n",
      "          3.2178e-02,  3.3541e-02, -5.8008e-02, -2.7956e-02,  4.8189e-02,\n",
      "          7.1868e-02,  4.1491e-02, -9.4137e-02, -6.8795e-02,  3.6024e-02,\n",
      "          2.5259e-02, -1.1728e-02, -1.0429e-01,  1.6456e-02, -3.3398e-02,\n",
      "          3.0465e-02, -9.7784e-02, -9.8470e-02,  8.7432e-02,  5.9022e-02,\n",
      "         -9.1799e-02, -4.2293e-02, -7.5805e-02,  9.0428e-02, -1.0667e-01,\n",
      "         -9.6908e-02, -5.7647e-02,  7.8367e-02,  2.5562e-02, -8.2960e-02,\n",
      "          3.1100e-02,  6.8778e-02, -5.2145e-02,  9.6367e-02,  5.2899e-02,\n",
      "         -6.0953e-02, -1.0097e-01,  3.7121e-02,  5.8452e-02, -1.0621e-01,\n",
      "          4.9362e-02, -4.5636e-02, -8.6792e-03,  4.2129e-02,  6.9461e-02,\n",
      "         -2.2507e-02,  5.5091e-03, -3.6064e-02,  7.3937e-02, -4.2289e-02,\n",
      "          7.6221e-02, -4.1973e-02, -7.0056e-02, -4.2894e-02,  8.9173e-02,\n",
      "          1.0158e-02, -2.9389e-02, -3.0722e-02,  9.2637e-02, -8.8597e-02,\n",
      "         -1.0431e-01,  1.5771e-04, -8.5114e-02,  5.2100e-02,  6.7411e-02,\n",
      "          1.0741e-01,  6.5751e-02, -3.6441e-02,  1.0077e-01, -3.5306e-03,\n",
      "          8.6527e-02,  2.4651e-03,  3.0720e-02,  4.4605e-02,  6.3753e-02,\n",
      "         -6.3625e-02, -7.2027e-02, -6.8800e-02, -8.6789e-02, -6.2294e-02,\n",
      "         -3.4920e-02,  8.6485e-02,  5.9430e-02,  2.7075e-02, -6.9181e-02,\n",
      "          5.4111e-02, -4.1507e-02, -9.2646e-02, -1.0209e-03,  2.8994e-02,\n",
      "         -4.2823e-02, -6.1020e-02,  6.9059e-02,  1.3003e-03,  5.2149e-02,\n",
      "          3.7870e-03,  3.2446e-02,  5.5358e-02, -9.3508e-02,  2.0996e-02,\n",
      "          5.6818e-02, -2.3836e-02,  5.1545e-02, -8.1245e-02,  6.0891e-02,\n",
      "          6.6439e-02, -4.8219e-02, -3.7182e-03,  4.0724e-02, -6.2817e-02,\n",
      "         -8.5704e-02, -7.2592e-02, -6.5164e-02,  1.0372e-01, -2.8995e-02,\n",
      "          4.9243e-02, -1.0764e-01,  5.3695e-02, -1.0026e-01,  5.1136e-02,\n",
      "          3.8766e-03,  1.0597e-01, -3.0280e-02,  1.2771e-02,  7.8948e-02,\n",
      "         -1.0158e-01,  1.2209e-02, -5.7221e-02, -4.9592e-02,  8.2356e-02,\n",
      "          5.9864e-02,  7.8292e-02, -2.3299e-02, -1.1046e-02,  9.2554e-02,\n",
      "         -2.0472e-02, -2.7926e-03, -1.3614e-02, -1.2816e-02,  6.1730e-02,\n",
      "         -3.7059e-02,  7.3909e-02, -5.7170e-02,  9.3262e-02,  1.4819e-02,\n",
      "          9.8572e-02, -9.9990e-03, -8.5076e-02,  2.3080e-02,  3.0720e-02,\n",
      "          8.5047e-02,  2.2645e-02, -6.7783e-02,  8.4744e-03, -7.6923e-04,\n",
      "          1.6700e-02,  1.8676e-02,  1.9480e-05, -9.3367e-02, -9.3192e-02,\n",
      "         -4.7869e-02,  5.6529e-02,  1.0535e-01,  7.2495e-02, -6.7717e-02,\n",
      "         -5.7510e-02,  3.0098e-02,  3.7159e-03, -4.7605e-02,  1.0498e-01,\n",
      "         -3.9086e-02,  2.9325e-02, -4.5313e-02, -7.9408e-02,  5.0925e-03,\n",
      "          5.7667e-02,  8.1484e-02,  2.2782e-02,  9.5026e-02, -8.5263e-02,\n",
      "          9.0188e-02, -1.0516e-01, -3.2800e-02, -9.7218e-02,  6.7618e-02,\n",
      "          3.2107e-02, -3.5961e-02,  3.2849e-02,  5.4117e-02, -7.8887e-02,\n",
      "         -3.4615e-02, -1.0387e-02,  4.0376e-02, -8.2551e-04, -9.6270e-02,\n",
      "         -3.4460e-02, -6.8678e-02, -8.0406e-02, -2.3923e-02, -1.3459e-03,\n",
      "          2.9202e-02,  2.5035e-02, -1.0407e-01, -1.1660e-02, -4.4107e-02,\n",
      "         -8.6119e-02, -4.0981e-02,  2.4350e-02, -9.3640e-02,  1.0076e-01,\n",
      "          5.1250e-02,  9.4563e-02, -5.0553e-02,  8.1240e-02,  6.3897e-02,\n",
      "         -6.5297e-02,  6.0674e-02, -6.1499e-02,  1.0547e-01, -6.8049e-02,\n",
      "          7.0573e-03, -6.8807e-03,  8.1054e-02,  2.0197e-03,  1.2783e-03,\n",
      "         -2.8146e-02,  4.5186e-02, -9.0158e-02, -9.4287e-02,  6.9926e-02,\n",
      "          9.5633e-02,  7.4065e-02,  7.8469e-02, -1.0510e-01,  5.3719e-02,\n",
      "         -7.1382e-02, -1.6843e-02, -1.5450e-02,  9.8687e-02, -1.6604e-03,\n",
      "          6.5039e-03, -1.0662e-01,  5.4173e-03, -2.6165e-02, -2.7180e-02,\n",
      "         -3.9056e-02,  5.6254e-02, -2.6891e-02,  6.5929e-02,  2.3223e-02,\n",
      "          8.5512e-02,  5.9459e-02,  3.1455e-02, -4.7944e-02,  8.3162e-02,\n",
      "          8.7985e-02, -4.0231e-02, -1.0672e-01, -6.6191e-02, -8.7680e-02,\n",
      "          6.2909e-02,  8.3229e-02, -8.2630e-02, -9.3305e-02,  2.0559e-02,\n",
      "         -8.8700e-02,  6.1612e-03,  9.7614e-03, -1.1306e-02,  5.9765e-02,\n",
      "         -8.5916e-02,  6.5073e-02,  5.5810e-02,  7.9709e-02,  2.1150e-02,\n",
      "         -7.4109e-02, -6.9663e-02, -5.7219e-02, -8.5636e-02, -2.4154e-02,\n",
      "         -3.8548e-04,  2.9369e-02, -1.7356e-02, -1.4455e-02,  1.9271e-02,\n",
      "          8.4938e-02,  2.3666e-02, -8.0599e-02, -3.0737e-02, -8.9209e-02,\n",
      "          3.6280e-02,  4.8822e-03,  6.1616e-02,  7.3097e-02, -6.2279e-02,\n",
      "         -1.1245e-02,  9.5853e-02, -9.2190e-02,  2.4200e-02, -9.4179e-02,\n",
      "         -6.0272e-02,  8.6748e-03, -4.3732e-02,  3.3600e-02, -4.7612e-03,\n",
      "         -4.9981e-02,  4.8551e-02, -6.5515e-02, -1.5551e-02,  2.5918e-02,\n",
      "         -9.2481e-03,  1.0290e-01,  5.6810e-02, -4.9057e-02, -6.1318e-02,\n",
      "          1.0195e-01,  7.0040e-02,  4.6636e-02,  2.1339e-02,  9.6702e-02,\n",
      "         -9.3001e-02, -5.2663e-02, -9.1873e-02, -1.1207e-02,  1.0665e-01,\n",
      "          2.2221e-02, -2.7521e-02,  6.0438e-02,  3.6151e-02,  6.9436e-02,\n",
      "         -3.3529e-02,  4.4109e-03, -1.8406e-02,  6.2203e-02,  2.7499e-04,\n",
      "          1.0251e-01,  5.7681e-02,  7.7821e-02,  9.0169e-02, -6.7003e-02,\n",
      "          8.9305e-02, -6.5325e-02, -4.5756e-02, -6.5898e-02, -7.8954e-02,\n",
      "          7.5466e-02, -7.4423e-02, -2.6091e-02, -1.3244e-03, -7.1527e-02,\n",
      "          9.2487e-02, -3.8302e-02, -4.2866e-02,  1.1493e-02, -6.7526e-02,\n",
      "         -7.8712e-02,  4.1863e-02,  9.7878e-02, -4.8801e-02, -9.5125e-02,\n",
      "         -9.2665e-02,  1.0752e-01, -4.5305e-02,  4.4976e-02,  2.2571e-02,\n",
      "         -1.8944e-02, -1.0053e-01]])), ('bias', tensor([0.0075]))])\n"
     ]
    }
   ],
   "source": [
    "print(model.fc.state_dict())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 [train]: 100%|██████████| 349/349 [00:31<00:00, 11.20batch/s, batch_loss=2.03] \n",
      "Epoch 0 [val]: 100%|██████████| 44/44 [00:01<00:00, 26.36batch/s, batch_loss=0.646]\n",
      "Epoch 1 [train]: 100%|██████████| 349/349 [00:30<00:00, 11.37batch/s, batch_loss=1.35] \n",
      "Epoch 1 [val]: 100%|██████████| 44/44 [00:01<00:00, 22.60batch/s, batch_loss=4.09] \n",
      "Epoch 2 [train]: 100%|██████████| 349/349 [00:30<00:00, 11.38batch/s, batch_loss=2.84] \n",
      "Epoch 2 [val]: 100%|██████████| 44/44 [00:02<00:00, 19.43batch/s, batch_loss=0.451]\n",
      "Epoch 3 [train]: 100%|██████████| 349/349 [00:31<00:00, 11.23batch/s, batch_loss=3.57] \n",
      "Epoch 3 [val]: 100%|██████████| 44/44 [00:01<00:00, 23.88batch/s, batch_loss=1.61]\n",
      "Epoch 4 [train]: 100%|██████████| 349/349 [00:31<00:00, 10.99batch/s, batch_loss=1.49] \n",
      "Epoch 4 [val]: 100%|██████████| 44/44 [00:01<00:00, 26.44batch/s, batch_loss=1.07] \n",
      "Epoch 5 [train]: 100%|██████████| 349/349 [00:30<00:00, 11.30batch/s, batch_loss=0.899]\n",
      "Epoch 5 [val]: 100%|██████████| 44/44 [00:02<00:00, 20.74batch/s, batch_loss=1.43] \n",
      "Epoch 6 [train]: 100%|██████████| 349/349 [00:31<00:00, 11.18batch/s, batch_loss=2.04] \n",
      "Epoch 6 [val]: 100%|██████████| 44/44 [00:02<00:00, 20.83batch/s, batch_loss=1.2]  \n",
      "Epoch 7 [train]: 100%|██████████| 349/349 [00:31<00:00, 11.03batch/s, batch_loss=10.5] \n",
      "Epoch 7 [val]: 100%|██████████| 44/44 [00:01<00:00, 23.48batch/s, batch_loss=1.68] \n",
      "Epoch 8 [train]: 100%|██████████| 349/349 [00:31<00:00, 11.07batch/s, batch_loss=0.572]\n",
      "Epoch 8 [val]: 100%|██████████| 44/44 [00:01<00:00, 22.09batch/s, batch_loss=1.49] \n",
      "Epoch 9 [train]: 100%|██████████| 349/349 [00:30<00:00, 11.36batch/s, batch_loss=6.74] \n",
      "Epoch 9 [val]: 100%|██████████| 44/44 [00:02<00:00, 20.83batch/s, batch_loss=0.79] \n",
      "Epoch 10 [train]: 100%|██████████| 349/349 [00:31<00:00, 11.21batch/s, batch_loss=1.62] \n",
      "Epoch 10 [val]: 100%|██████████| 44/44 [00:02<00:00, 20.70batch/s, batch_loss=1.13] \n",
      "Epoch 11 [train]: 100%|██████████| 349/349 [00:31<00:00, 11.16batch/s, batch_loss=1.33] \n",
      "Epoch 11 [val]: 100%|██████████| 44/44 [00:01<00:00, 22.15batch/s, batch_loss=0.912]\n",
      "Epoch 12 [train]: 100%|██████████| 349/349 [00:31<00:00, 11.15batch/s, batch_loss=1.37] \n",
      "Epoch 12 [val]: 100%|██████████| 44/44 [00:02<00:00, 20.48batch/s, batch_loss=13.6] \n",
      "Epoch 13 [train]: 100%|██████████| 349/349 [00:30<00:00, 11.60batch/s, batch_loss=0.695]\n",
      "Epoch 13 [val]: 100%|██████████| 44/44 [00:02<00:00, 20.62batch/s, batch_loss=1.97] \n",
      "Epoch 14 [train]: 100%|██████████| 349/349 [00:30<00:00, 11.29batch/s, batch_loss=3.85] \n",
      "Epoch 14 [val]: 100%|██████████| 44/44 [00:02<00:00, 16.91batch/s, batch_loss=3.65] \n",
      "Epoch 15 [train]: 100%|██████████| 349/349 [00:31<00:00, 11.03batch/s, batch_loss=3.16] \n",
      "Epoch 15 [val]: 100%|██████████| 44/44 [00:01<00:00, 22.61batch/s, batch_loss=1.03] \n",
      "Epoch 16 [train]: 100%|██████████| 349/349 [00:31<00:00, 10.93batch/s, batch_loss=3.65] \n",
      "Epoch 16 [val]: 100%|██████████| 44/44 [00:02<00:00, 19.53batch/s, batch_loss=1.47] \n",
      "Epoch 17 [train]: 100%|██████████| 349/349 [00:31<00:00, 10.98batch/s, batch_loss=0.694]\n",
      "Epoch 17 [val]: 100%|██████████| 44/44 [00:01<00:00, 22.03batch/s, batch_loss=0.982]\n",
      "Epoch 18 [train]: 100%|██████████| 349/349 [00:32<00:00, 10.89batch/s, batch_loss=2]    \n",
      "Epoch 18 [val]: 100%|██████████| 44/44 [00:01<00:00, 22.20batch/s, batch_loss=0.932]\n",
      "Epoch 19 [train]: 100%|██████████| 349/349 [00:31<00:00, 10.99batch/s, batch_loss=0.737]\n",
      "Epoch 19 [val]: 100%|██████████| 44/44 [00:02<00:00, 20.34batch/s, batch_loss=2.02] \n"
     ]
    }
   ],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# An epoch is one complete pass of the training dataset through the network\n",
    "NB_EPOCHS = 20\n",
    "\n",
    "# Loop over the epochs\n",
    "for epoch in range(NB_EPOCHS):\n",
    "    \n",
    "    # Training\n",
    "    train_loss = 0.\n",
    "    \n",
    "    # Configure the model for training\n",
    "    # (good practice, only necessary if the model operates differently for\n",
    "    # training and validation)\n",
    "    model.train()\n",
    "    \n",
    "    # Add a progress bar\n",
    "    train_loader_pbar = tqdm(train_loader, unit=\"batch\")\n",
    "    \n",
    "    # Loop over the training batches\n",
    "    for images, traversal_scores in train_loader_pbar:\n",
    "        \n",
    "        # Print the epoch and training mode\n",
    "        train_loader_pbar.set_description(f\"Epoch {epoch} [train]\")\n",
    "        \n",
    "        # Move images and traversal scores to GPU (if available)\n",
    "        images = images.to(device)\n",
    "        traversal_scores = traversal_scores.to(device)\n",
    "        \n",
    "        # Zero out gradients before each backpropagation pass, to avoid that\n",
    "        # they accumulate\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform forward pass\n",
    "        predicted_traversal_scores = model(images)\n",
    "        \n",
    "        # Compute loss \n",
    "        loss = criterion(predicted_traversal_scores, traversal_scores)\n",
    "        \n",
    "        # Print the batch loss next to the progress bar\n",
    "        train_loader_pbar.set_postfix(batch_loss=loss.item())\n",
    "        \n",
    "        # Perform backpropagation (compute gradients)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Adjust parameters based on gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate batch loss to average over the epoch\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    # Validation\n",
    "    val_loss = 0.\n",
    "    \n",
    "    # Configure the model for testing\n",
    "    model.eval()\n",
    "    \n",
    "    # Add a progress bar\n",
    "    val_loader_pbar = tqdm(val_loader, unit=\"batch\")\n",
    "    \n",
    "    # Loop over the validation batches\n",
    "    for images, traversal_scores in val_loader_pbar:\n",
    "        \n",
    "        # Print the epoch and validation mode\n",
    "        val_loader_pbar.set_description(f\"Epoch {epoch} [val]\")\n",
    "        \n",
    "        # Move images and traversal scores to GPU (if available)\n",
    "        images = images.to(device)\n",
    "        traversal_scores = traversal_scores.to(device)\n",
    "        \n",
    "        # Perform forward pass (only, no backpropagation)\n",
    "        predicted_traversal_scores = model(images)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(predicted_traversal_scores, traversal_scores)\n",
    "        \n",
    "        # Print the batch loss next to the progress bar\n",
    "        val_loader_pbar.set_postfix(batch_loss=loss.item())\n",
    "        \n",
    "        # Accumulate batch loss to average over the epoch\n",
    "        val_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    # # Display the computed losses\n",
    "    # print(f\"Epoch {epoch}: Train Loss: {train_loss/len(train_loader)}\\\n",
    "    #       Validation Loss: {val_loss/len(val_loader)}\")\n",
    "    # loss_values[0, epoch] = train_loss/len(train_loader)\n",
    "    # loss_values[1, epoch] = val_loss/len(val_loader)\n",
    "    \n",
    "    # Add the losses to TensorBoard\n",
    "    tensorboard.add_scalar(\"train_loss\", train_loss/len(train_loader), epoch)\n",
    "    tensorboard.add_scalar(\"val_loss\", val_loss/len(val_loader), epoch)\n",
    "\n",
    "# Close TensorBoard\n",
    "tensorboard.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.7003943256356497\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_loss = 0.\n",
    "\n",
    "denormalize = transforms.Compose([\n",
    "    transforms.Normalize(\n",
    "        mean=[0., 0., 0.],\n",
    "        std=[1/0.229, 1/0.224, 1/0.225]\n",
    "    ),\n",
    "    transforms.Normalize(\n",
    "        mean=[-0.485, -0.456, -0.406],\n",
    "        std=[1., 1., 1.]\n",
    "    ),\n",
    "    transforms.ToPILImage(),\n",
    "])\n",
    "\n",
    "# Loop over the testing batches\n",
    "for images, traversal_scores in test_loader:\n",
    "    \n",
    "    images = images.to(device)\n",
    "    traversal_scores = traversal_scores.to(device)\n",
    "    \n",
    "    # Perform forward pass\n",
    "    predicted_traversal_scores = model(images)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(predicted_traversal_scores, traversal_scores)\n",
    "    \n",
    "    # Accumulate batch loss to average of the entire testing set\n",
    "    test_loss += loss.item()\n",
    "    # print(loss.item())\n",
    "    \n",
    "    # for i in range(images.shape[0]):\n",
    "    #     plt.imshow(denormalize(images[i]))\n",
    "\n",
    "print(f\"Test loss: {test_loss/len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      " tensor([[ 1.5458],\n",
      "        [ 1.2313],\n",
      "        [ 0.0577],\n",
      "        [ 1.8463],\n",
      "        [ 1.2239],\n",
      "        [ 1.9654],\n",
      "        [ 1.7917],\n",
      "        [ 1.2777],\n",
      "        [ 1.6487],\n",
      "        [ 1.2184],\n",
      "        [ 1.5767],\n",
      "        [ 1.7410],\n",
      "        [-0.0196],\n",
      "        [ 1.4327],\n",
      "        [ 1.4542],\n",
      "        [ 0.9224]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Ground truth:\n",
      " tensor([[0.3861],\n",
      "        [1.3383],\n",
      "        [0.2454],\n",
      "        [3.4075],\n",
      "        [0.7408],\n",
      "        [3.7817],\n",
      "        [1.2879],\n",
      "        [1.4589],\n",
      "        [0.7179],\n",
      "        [1.1616],\n",
      "        [2.4016],\n",
      "        [1.8924],\n",
      "        [0.7813],\n",
      "        [2.4893],\n",
      "        [1.2375],\n",
      "        [1.1009]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "images, traversal_scores = next(iter(test_loader))\n",
    "\n",
    "images = images.to(device)\n",
    "traversal_scores = traversal_scores.to(device)\n",
    "\n",
    "predicted_traversal_scores = model(images)\n",
    "\n",
    "print(\"Prediction:\\n\", predicted_traversal_scores)\n",
    "print(\"Ground truth:\\n\", traversal_scores)\n",
    "\n",
    "# print(predicted_traversal_scores-traversal_scores)\n",
    "\n",
    "# predicted_traversal_scores = predicted_traversal_scores.to(\"cpu\").detach().numpy()\n",
    "# plt.hist(predicted_traversal_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([341., 206.,  89.,  31.,  16.,   6.,   4.,   2.,   0.,   2.]),\n",
       " array([2.89384684e-06, 1.22046545e-02, 2.44064145e-02, 3.66081744e-02,\n",
       "        4.88099344e-02, 6.10116944e-02, 7.32134581e-02, 8.54152143e-02,\n",
       "        9.76169780e-02, 1.09818734e-01, 1.22020498e-01], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traversal_scores_train = []\n",
    "\n",
    "for _, score in test_set:\n",
    "    traversal_scores_train.append(score[0])\n",
    "    \n",
    "# print(traversal_scores_train)\n",
    "plt.hist(traversal_scores_train, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(44*16-697)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model parameters\n",
    "torch.save(model.state_dict(), \"resnet18_fine_tuned.params\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
